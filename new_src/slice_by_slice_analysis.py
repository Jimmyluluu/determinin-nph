#!/usr/bin/env python3
"""
é€åˆ‡ç‰‡ Evans Index åˆ†ææ¨¡çµ„
åœ¨ç¢ºå®šçš„å‰è§’ç¯„åœå…§ç‚ºæ¯å€‹åˆ‡ç‰‡è¨ˆç®— Evans Index
"""
import os
import json
import numpy as np
import nibabel as nib
from typing import Dict, List, Tuple, Optional
from utils import calculate_evans_index
from image_processing import find_skull_segment
from visualization import generate_evans_slice_screenshot


def analyze_slices_in_frontal_horn_range(nii_path: str, brain_mask_path: str, dataset_name: str = "",
                                        max_reasonable_width: int = 200, occupancy_threshold: float = 0.6) -> List[Dict]:
    """
    åœ¨å‰è§’ç¯„åœå…§åˆ†ææ‰€æœ‰åˆ‡ç‰‡çš„ Evans Index

    Parameters:
        nii_path (str): è…¦å®¤é®ç½©è·¯å¾‘
        brain_mask_path (str): è…¦éƒ¨é®ç½©è·¯å¾‘
        dataset_name (str): è³‡æ–™é›†åç¨±
        max_reasonable_width (int): æœ€å¤§åˆç†å¯¬åº¦
        occupancy_threshold (float): ä½”æœ‰ç‡é–¾å€¼

    Returns:
        List[Dict]: æ‰€æœ‰æœ‰æ•ˆåˆ‡ç‰‡çš„æ¸¬é‡çµæœ
    """
    img = nib.load(nii_path)
    mask_data = img.get_fdata()
    binary = (mask_data > 0).astype(np.uint8)

    X, Y, Z = binary.shape
    slice_results = []

    # æª¢æŸ¥é®ç½©æ˜¯å¦æœ‰å…§å®¹
    total_pixels = np.count_nonzero(binary)
    if total_pixels == 0:
        print(f"âŒ {dataset_name}: è…¦å®¤é®ç½©å®Œå…¨ç‚ºç©º")
        return slice_results

    # æ‰¾å‡ºè…¦å®¤çš„ Z è»¸ç¯„åœï¼ˆé‡ç”¨åŸé‚è¼¯ï¼‰
    z_coords = []
    for z in range(Z):
        if np.count_nonzero(binary[:, :, z]) > 0:
            z_coords.append(z)

    if not z_coords:
        return slice_results

    z_min, z_max = min(z_coords), max(z_coords)
    z_range = z_max - z_min

    # å‰è§’å€åŸŸï¼šZ è»¸å‰éƒ¨ (z/3 åˆ° z)
    if z_range > 0:
        target_z_start = int(z_min + z_range / 3)
        target_z_end = z_max
    else:
        target_z_start = target_z_end = z_min

    # æ ¹æ“šè³‡æ–™ä¾†æºæ±ºå®š Y è»¸æœç´¢ç¯„åœ
    y_mid = Y // 2
    is_data_series = dataset_name.startswith('data_')

    if is_data_series:
        y_search_range = range(0, y_mid)
        search_description = "ä¸‹åŠéƒ¨ (data ç³»åˆ—)"
    else:
        y_search_range = range(y_mid, Y)
        search_description = "ä¸ŠåŠéƒ¨ (ç·¨è™Ÿç³»åˆ—)"

    print(f"ğŸ” {dataset_name}: åœ¨å‰è§’ç¯„åœ Z={target_z_start}-{target_z_end} ä¸­åˆ†ææ‰€æœ‰åˆ‡ç‰‡...")

    # é€åˆ‡ç‰‡åˆ†æ
    for z in range(target_z_start, min(target_z_end + 1, Z)):
        slice_ = binary[:, :, z]

        if np.count_nonzero(slice_) == 0:
            continue

        # æ‰¾è©²åˆ‡ç‰‡çš„æœ€ä½³è…¦å®¤æ¸¬é‡
        best_ventricle = {'width': 0, 'z': z, 'y': None, 'x1': None, 'x2': None, 'occupancy': 0}

        for y in y_search_range:
            col = slice_[:, y]
            xs = np.where(col > 0)[0]

            if xs.size < 2:
                continue

            x1, x2 = xs.min(), xs.max()
            width = x2 - x1

            # æª¢æŸ¥å¯¬åº¦åˆç†æ€§
            if width > max_reasonable_width or width < 5:
                continue

            # æª¢æŸ¥ä½”æœ‰ç‡
            occupancy = col[x1:x2+1].sum() / (width + 1) if width > 0 else 0

            # æ›´æ–°æœ€ä½³æ¸¬é‡
            if occupancy >= occupancy_threshold and width > best_ventricle['width']:
                best_ventricle.update({
                    'width': int(width),
                    'z': int(z),
                    'y': int(y),
                    'x1': int(x1),
                    'x2': int(x2),
                    'occupancy': float(occupancy)
                })

        # å¦‚æœæ‰¾åˆ°æœ‰æ•ˆçš„è…¦å®¤æ¸¬é‡ï¼Œå°‹æ‰¾å°æ‡‰çš„é¡±éª¨æ¸¬é‡
        if best_ventricle['width'] > 0:
            try:
                skull_segment = find_skull_segment(brain_mask_path, z, best_ventricle['y'])

                # è¨ˆç®— Evans Index
                evans_results = calculate_evans_index(
                    best_ventricle['width'],
                    skull_segment['width']
                )

                # è¨˜éŒ„åˆ‡ç‰‡çµæœ
                slice_result = {
                    'slice_z': z,
                    'ventricle_segment': best_ventricle,
                    'skull_segment': skull_segment,
                    'evans_analysis': evans_results
                }

                slice_results.append(slice_result)

            except Exception as e:
                print(f"âš ï¸ {dataset_name} Z={z}: é¡±éª¨æ¸¬é‡å¤±æ•— - {str(e)}")
                continue

    print(f"âœ… {dataset_name}: åœ¨ç¯„åœå…§æ‰¾åˆ° {len(slice_results)} å€‹æœ‰æ•ˆåˆ‡ç‰‡")
    return slice_results


def generate_slice_screenshots_for_case(case_name: str, original_path: str, ventricle_path: str,
                                       brain_mask_path: str, slice_results: List[Dict],
                                       output_base_dir: str) -> List[str]:
    """
    ç‚ºæ¡ˆä¾‹çš„æ‰€æœ‰åˆ‡ç‰‡ç”Ÿæˆæˆªåœ–

    Returns:
        List[str]: ç”Ÿæˆçš„æˆªåœ–è·¯å¾‘åˆ—è¡¨
    """
    case_output_dir = os.path.join(output_base_dir, case_name)
    os.makedirs(case_output_dir, exist_ok=True)

    screenshot_paths = []

    for slice_data in slice_results:
        z_slice = slice_data['slice_z']
        evans_index = slice_data['evans_analysis']['evans_index']

        # ç”ŸæˆåŒ…å« Evans Index çš„æª”å
        screenshot_filename = f"slice_{z_slice:03d}_evans_{evans_index:.4f}.png"
        screenshot_path = os.path.join(case_output_dir, screenshot_filename)

        # ä½¿ç”¨ç¾æœ‰çš„æˆªåœ–ç”Ÿæˆå‡½æ•¸ï¼Œä½†æŒ‡å®šç‰¹å®šçš„è¼¸å‡ºè·¯å¾‘
        try:
            success = generate_evans_slice_screenshot(
                case_name=f"{case_name}_slice_{z_slice}",
                original_path=original_path,
                ventricle_path=ventricle_path,
                brain_mask_path=brain_mask_path,
                ventricle_coords=slice_data['ventricle_segment'],
                skull_coords=slice_data['skull_segment'],
                output_dir=case_output_dir
            )

            if success:
                # é‡å‘½åæª”æ¡ˆç‚ºæˆ‘å€‘æƒ³è¦çš„æ ¼å¼
                old_path = os.path.join(case_output_dir, f"{case_name}_slice_{z_slice}_evans_slice.png")
                if os.path.exists(old_path):
                    os.rename(old_path, screenshot_path)
                    screenshot_paths.append(screenshot_path)

        except Exception as e:
            print(f"âŒ {case_name} åˆ‡ç‰‡ {z_slice} æˆªåœ–ç”Ÿæˆå¤±æ•—: {str(e)}")
            continue

    return screenshot_paths


def save_case_slice_data(case_name: str, slice_results: List[Dict], output_base_dir: str) -> str:
    """
    ä¿å­˜æ¡ˆä¾‹çš„æ‰€æœ‰åˆ‡ç‰‡æ•¸æ“šåˆ° JSON æª”æ¡ˆï¼ˆæ’é™¤è©²æ¡ˆä¾‹å…§éƒ¨çš„é›¢ç¾¤åˆ‡ç‰‡ï¼‰
    """
    case_output_dir = os.path.join(output_base_dir, case_name)
    os.makedirs(case_output_dir, exist_ok=True)

    json_path = os.path.join(case_output_dir, "slices_data.json")

    if not slice_results:
        data = {
            "case_name": case_name,
            "total_slices": 0,
            "slice_range": {"min_z": None, "max_z": None},
            "evans_index_stats": {"min": None, "max": None, "mean": None, "std": None},
            "slice_details": []
        }
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return json_path

    # æå–æ‰€æœ‰ Evans Index å€¼
    evans_indices = [s['evans_analysis']['evans_index'] for s in slice_results]

    # è¨ˆç®—åŸå§‹çµ±è¨ˆ
    original_stats = {
        "min": min(evans_indices),
        "max": max(evans_indices),
        "mean": np.mean(evans_indices),
        "std": np.std(evans_indices),
        "count": len(evans_indices)
    }

    # ä½¿ç”¨ IQR æ–¹æ³•æª¢æ¸¬è©²æ¡ˆä¾‹å…§éƒ¨çš„é›¢ç¾¤åˆ‡ç‰‡
    filtered_indices = evans_indices
    outlier_slices = []

    if len(evans_indices) > 4:  # éœ€è¦è¶³å¤ çš„åˆ‡ç‰‡æ•¸æ‰é€²è¡Œé›¢ç¾¤å€¼æª¢æ¸¬
        q1 = np.percentile(evans_indices, 25)
        q3 = np.percentile(evans_indices, 75)
        iqr = q3 - q1

        if iqr > 0:  # é¿å…æ‰€æœ‰å€¼éƒ½ç›¸åŒçš„æƒ…æ³
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr

            # åˆ†é›¢æ­£å¸¸åˆ‡ç‰‡å’Œé›¢ç¾¤åˆ‡ç‰‡
            filtered_indices = []
            for i, slice_result in enumerate(slice_results):
                evans_idx = slice_result['evans_analysis']['evans_index']
                if lower_bound <= evans_idx <= upper_bound:
                    filtered_indices.append(evans_idx)
                else:
                    outlier_info = {
                        "slice_z": slice_result['slice_z'],
                        "evans_index": evans_idx,
                        "reason": "below_threshold" if evans_idx < lower_bound else "above_threshold"
                    }
                    outlier_slices.append(outlier_info)

    # è¨ˆç®—æ’é™¤é›¢ç¾¤å€¼å¾Œçš„çµ±è¨ˆ
    filtered_stats = {
        "min": min(filtered_indices) if filtered_indices else None,
        "max": max(filtered_indices) if filtered_indices else None,
        "mean": np.mean(filtered_indices) if filtered_indices else None,
        "std": np.std(filtered_indices) if filtered_indices else None,
        "count": len(filtered_indices)
    }

    # æ•´ç†æ•¸æ“š
    data = {
        "case_name": case_name,
        "total_slices": len(slice_results),
        "slice_range": {
            "min_z": min(s['slice_z'] for s in slice_results),
            "max_z": max(s['slice_z'] for s in slice_results)
        },
        "evans_index_stats": filtered_stats,  # ä½¿ç”¨æ’é™¤é›¢ç¾¤å€¼å¾Œçš„çµ±è¨ˆ
        "evans_index_stats_original": original_stats,  # ä¿ç•™åŸå§‹çµ±è¨ˆ
        "outlier_detection": {
            "outlier_slices": outlier_slices,
            "outlier_count": len(outlier_slices),
            "detection_method": "IQR_1.5" if len(evans_indices) > 4 else "disabled_insufficient_data"
        },
        "slice_details": slice_results
    }

    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“Š {case_name}: ç¸½åˆ‡ç‰‡ {len(slice_results)} å€‹ï¼Œæ’é™¤é›¢ç¾¤å€¼ {len(outlier_slices)} å€‹ï¼Œæœ‰æ•ˆåˆ‡ç‰‡ {len(filtered_indices)} å€‹")

    return json_path


def run_slice_by_slice_analysis_for_case(case_name: str, case_paths: Dict,
                                        occupancy_threshold: float = 0.6,
                                        output_base_dir: str = "result/detailed_slices") -> Optional[Dict]:
    """
    ç‚ºå–®å€‹æ¡ˆä¾‹åŸ·è¡Œé€åˆ‡ç‰‡åˆ†æ
    """
    try:
        print(f"\nğŸ” é–‹å§‹é€åˆ‡ç‰‡åˆ†æ: {case_name}")

        # åˆ†ææ‰€æœ‰åˆ‡ç‰‡
        slice_results = analyze_slices_in_frontal_horn_range(
            case_paths['ventricles'],
            case_paths['brain_mask'],
            case_name,
            occupancy_threshold=occupancy_threshold
        )

        if not slice_results:
            print(f"âŒ {case_name}: æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åˆ‡ç‰‡")
            return None

        # ç”Ÿæˆæˆªåœ–
        screenshot_paths = generate_slice_screenshots_for_case(
            case_name,
            case_paths['original'],
            case_paths['ventricles'],
            case_paths['brain_mask'],
            slice_results,
            output_base_dir
        )

        # ä¿å­˜æ•¸æ“š
        json_path = save_case_slice_data(case_name, slice_results, output_base_dir)

        # è®€å–ä¿å­˜çš„æ•¸æ“šä»¥ç²å–æ’é™¤é›¢ç¾¤å€¼å¾Œçš„çµ±è¨ˆ
        with open(json_path, 'r', encoding='utf-8') as f:
            saved_data = json.load(f)

        summary = {
            "case_name": case_name,
            "total_slices": len(slice_results),
            "effective_slices": saved_data['evans_index_stats']['count'],  # æ’é™¤é›¢ç¾¤å€¼å¾Œçš„æœ‰æ•ˆåˆ‡ç‰‡æ•¸
            "outlier_slices": saved_data['outlier_detection']['outlier_count'],  # é›¢ç¾¤å€¼åˆ‡ç‰‡æ•¸
            "slice_range": {
                "min_z": min(s['slice_z'] for s in slice_results),
                "max_z": max(s['slice_z'] for s in slice_results)
            },
            "evans_index_stats": saved_data['evans_index_stats'],  # æ’é™¤é›¢ç¾¤å€¼å¾Œçš„çµ±è¨ˆ
            "evans_index_stats_original": saved_data['evans_index_stats_original'],  # åŸå§‹çµ±è¨ˆ
            "outlier_detection": saved_data['outlier_detection'],
            "screenshots_generated": len(screenshot_paths),
            "data_file": json_path
        }

        print(f"âœ… {case_name}: å®Œæˆ {len(slice_results)} å€‹åˆ‡ç‰‡åˆ†æï¼Œç”Ÿæˆ {len(screenshot_paths)} å¼µæˆªåœ–")
        return summary

    except Exception as e:
        print(f"âŒ {case_name}: é€åˆ‡ç‰‡åˆ†æå¤±æ•— - {str(e)}")
        return None


def generate_detailed_summary_report(all_case_summaries: List[Dict], output_path: str):
    """
    ç”Ÿæˆæ‰€æœ‰æ¡ˆä¾‹çš„é€åˆ‡ç‰‡åˆ†æè©³ç´°æ‘˜è¦å ±å‘Š
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("# é€åˆ‡ç‰‡ Evans Index åˆ†æè©³ç´°å ±å‘Š\n\n")
        f.write(f"ğŸ“… åˆ†ææ™‚é–“: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # éæ¿¾å‡ºæˆåŠŸåˆ†æçš„æ¡ˆä¾‹
        successful_cases = [s for s in all_case_summaries if s is not None]
        failed_count = len(all_case_summaries) - len(successful_cases)

        f.write("## ğŸ“Š æ•´é«”çµ±è¨ˆ\n\n")
        f.write(f"- **ç¸½å…±è™•ç†æ¡ˆä¾‹**: {len(all_case_summaries)} å€‹\n")
        f.write(f"- **æˆåŠŸåˆ†ææ¡ˆä¾‹**: {len(successful_cases)} å€‹\n")
        if failed_count > 0:
            f.write(f"- **åˆ†æå¤±æ•—æ¡ˆä¾‹**: {failed_count} å€‹\n")

        if successful_cases:
            total_slices = sum(s['total_slices'] for s in successful_cases)
            effective_slices = sum(s['effective_slices'] for s in successful_cases)
            total_outlier_slices = sum(s['outlier_slices'] for s in successful_cases)
            total_screenshots = sum(s['screenshots_generated'] for s in successful_cases)

            f.write(f"- **ç¸½å…±åˆ†æåˆ‡ç‰‡**: {total_slices} å€‹\n")
            f.write(f"- **æœ‰æ•ˆåˆ‡ç‰‡æ•¸** (æ’é™¤å€‹æ¡ˆå…§é›¢ç¾¤å€¼): {effective_slices} å€‹\n")
            f.write(f"- **å€‹æ¡ˆå…§é›¢ç¾¤åˆ‡ç‰‡**: {total_outlier_slices} å€‹\n")
            f.write(f"- **ç”Ÿæˆæˆªåœ–æ•¸é‡**: {total_screenshots} å¼µ\n")

            # Evans Index æ•´é«”çµ±è¨ˆï¼ˆä½¿ç”¨å„æ¡ˆä¾‹æ’é™¤é›¢ç¾¤å€¼å¾Œçš„å¹³å‡å€¼ï¼‰
            all_evans_means = []
            all_evans_mins = []
            all_evans_maxs = []

            for case in successful_cases:
                if case['evans_index_stats']['mean'] is not None:
                    all_evans_means.append(case['evans_index_stats']['mean'])
                    all_evans_mins.append(case['evans_index_stats']['min'])
                    all_evans_maxs.append(case['evans_index_stats']['max'])

            if all_evans_means:
                overall_mean = np.mean(all_evans_means)
                overall_std = np.std(all_evans_means)

                f.write(f"- **Evans Index ç¯„åœ**: {min(all_evans_mins):.4f} - {max(all_evans_maxs):.4f}\n")
                f.write(f"- **æ•´é«”å¹³å‡ Evans Index** (å„æ¡ˆä¾‹å‡å·²æ’é™¤å…§éƒ¨é›¢ç¾¤å€¼): {overall_mean:.4f} Â± {overall_std:.4f}\n\n")

        # å€‹æ¡ˆè©³ç´°è¡¨æ ¼
        f.write("## ğŸ“‹ å€‹æ¡ˆåˆ†æè©³æƒ…\n\n")
        if successful_cases:
            f.write("| æ¡ˆä¾‹ | ç¸½åˆ‡ç‰‡ | æœ‰æ•ˆåˆ‡ç‰‡ | é›¢ç¾¤åˆ‡ç‰‡ | Zç¯„åœ | Evans Index (æ’é™¤é›¢ç¾¤å€¼) | é¢¨éšªè©•ä¼° | æˆªåœ–æ•¸ |\n")
            f.write("|------|--------|----------|----------|-------|--------------------------|----------|--------|\n")

            for case in sorted(successful_cases, key=lambda x: x['case_name']):
                case_name = case['case_name']
                total_slices = case['total_slices']
                effective_slices = case['effective_slices']
                outlier_slices = case['outlier_slices']
                z_range = f"{case['slice_range']['min_z']}-{case['slice_range']['max_z']}"

                stats = case['evans_index_stats']
                if stats['mean'] is not None:
                    evans_display = f"{stats['mean']:.4f} ({stats['min']:.4f}-{stats['max']:.4f})"
                    mean_evans = stats['mean']
                else:
                    evans_display = "ç„¡æœ‰æ•ˆæ•¸æ“š"
                    mean_evans = 0

                # æ ¹æ“šå¹³å‡å€¼åˆ¤æ–·é¢¨éšª
                if mean_evans <= 0.25:
                    risk = "ğŸŸ¢ æ­£å¸¸"
                elif mean_evans <= 0.30:
                    risk = "ğŸŸ¡ å¯èƒ½æ“´å¤§"
                else:
                    risk = "ğŸ”´ è…¦å®¤æ“´å¤§"

                # æ¨™è¨˜æœ‰é›¢ç¾¤åˆ‡ç‰‡çš„æ¡ˆä¾‹
                if outlier_slices > 0:
                    risk += f" (æ’é™¤{outlier_slices}å€‹)"

                screenshot_count = case['screenshots_generated']

                f.write(f"| {case_name} | {total_slices} | {effective_slices} | {outlier_slices} | {z_range} | {evans_display} | {risk} | {screenshot_count} |\n")
        else:
            f.write("æ²’æœ‰æˆåŠŸåˆ†æçš„æ¡ˆä¾‹\n")

        # é¢¨éšªåˆ†é¡çµ±è¨ˆ
        if successful_cases:
            f.write("\n## ğŸ¯ é¢¨éšªåˆ†é¡çµ±è¨ˆ\n\n")

            # ä½¿ç”¨æ’é™¤å…§éƒ¨é›¢ç¾¤å€¼å¾Œçš„çµ±è¨ˆé€²è¡Œåˆ†é¡
            normal_cases = []
            mild_cases = []
            high_cases = []

            for case in successful_cases:
                if case['evans_index_stats']['mean'] is not None:
                    mean_evans = case['evans_index_stats']['mean']
                    if mean_evans <= 0.25:
                        normal_cases.append(case)
                    elif mean_evans <= 0.30:
                        mild_cases.append(case)
                    else:
                        high_cases.append(case)

            total = len(successful_cases)

            f.write(f"- **æ­£å¸¸ç¯„åœ (â‰¤ 0.25)**: {len(normal_cases)}/{total} ({len(normal_cases)/total*100:.1f}%)\n")
            f.write(f"- **å¯èƒ½æ“´å¤§ (0.25-0.30)**: {len(mild_cases)}/{total} ({len(mild_cases)/total*100:.1f}%)\n")
            f.write(f"- **è…¦å®¤æ“´å¤§ (> 0.30)**: {len(high_cases)}/{total} ({len(high_cases)/total*100:.1f}%)\n\n")

            # å…§éƒ¨é›¢ç¾¤å€¼çµ±è¨ˆ
            cases_with_outliers = [s for s in successful_cases if s['outlier_slices'] > 0]
            f.write(f"- **å«æœ‰å…§éƒ¨é›¢ç¾¤åˆ‡ç‰‡çš„æ¡ˆä¾‹**: {len(cases_with_outliers)}/{total} ({len(cases_with_outliers)/total*100:.1f}%)\n\n")

            # é«˜é¢¨éšªæ¡ˆä¾‹è©³æƒ…
            if high_cases:
                f.write("### ğŸ”´ è…¦å®¤æ“´å¤§æ¡ˆä¾‹è©³æƒ…\n\n")
                for case in sorted(high_cases, key=lambda x: x['evans_index_stats']['mean'], reverse=True):
                    stats = case['evans_index_stats']
                    original_stats = case['evans_index_stats_original']
                    outlier_count = case['outlier_slices']

                    f.write(f"- **{case['case_name']}**: ")
                    f.write(f"å¹³å‡ {stats['mean']:.4f} (æ’é™¤{outlier_count}å€‹é›¢ç¾¤åˆ‡ç‰‡å¾Œ), ")
                    f.write(f"ç¯„åœ {stats['min']:.4f}-{stats['max']:.4f}, ")
                    f.write(f"æ¨™æº–å·® {stats['std']:.4f}")
                    if outlier_count > 0:
                        f.write(f" [åŸå§‹å¹³å‡: {original_stats['mean']:.4f}]")
                    f.write("\n")

        # æª”æ¡ˆçµæ§‹èªªæ˜
        f.write("\n## ğŸ“ æª”æ¡ˆçµæ§‹èªªæ˜\n\n")
        f.write("```\n")
        f.write("result/detailed_slices/\n")
        f.write("â”œâ”€â”€ æ¡ˆä¾‹åç¨±/\n")
        f.write("â”‚   â”œâ”€â”€ slice_XXX_evans_Y.YYYY.png  # åˆ‡ç‰‡æˆªåœ–ï¼ˆæª”åå«Evans Indexï¼‰\n")
        f.write("â”‚   â””â”€â”€ slices_data.json            # è©²æ¡ˆä¾‹æ‰€æœ‰åˆ‡ç‰‡çš„è©³ç´°æ•¸æ“š\n")
        f.write("â””â”€â”€ detailed_summary.md             # æœ¬å ±å‘Šæª”æ¡ˆ\n")
        f.write("```\n\n")

        # èªªæ˜
        f.write("## ğŸ“– èªªæ˜\n\n")
        f.write("- æœ¬åˆ†æåœ¨å‰è§’ç¯„åœå…§å°æ¯å€‹æœ‰æ•ˆåˆ‡ç‰‡é€²è¡Œ Evans Index è¨ˆç®—\n")
        f.write("- åªåŒ…å«ä½”æœ‰ç‡ â‰¥ 0.6 ä¸”å¯¬åº¦åˆç†çš„åˆ‡ç‰‡\n")
        f.write("- **é›¢ç¾¤å€¼è™•ç†**: å°æ¯å€‹æ¡ˆä¾‹å…§éƒ¨çš„åˆ‡ç‰‡ä½¿ç”¨ IQR 1.5å€æ–¹æ³•æª¢æ¸¬ä¸¦æ’é™¤é›¢ç¾¤å€¼\n")
        f.write("- **çµ±è¨ˆåŸºæº–**: æ‰€æœ‰çµ±è¨ˆæ•¸æ“šï¼ˆå¹³å‡å€¼ã€ç¯„åœç­‰ï¼‰å‡åŸºæ–¼æ’é™¤å…§éƒ¨é›¢ç¾¤åˆ‡ç‰‡å¾Œçš„æœ‰æ•ˆåˆ‡ç‰‡\n")
        f.write("- æˆªåœ–æª”åæ ¼å¼: `slice_[Zåº§æ¨™]_evans_[Evans Indexå€¼].png`\n")
        f.write("- Evans Index æ­£å¸¸ç¯„åœ: â‰¤ 0.25ï¼›å¯èƒ½æ“´å¤§: 0.25-0.30ï¼›è…¦å®¤æ“´å¤§: > 0.30\n")
        f.write("- é¢¨éšªè©•ä¼°æ¬„ä½ä¸­çš„ \"(æ’é™¤Nå€‹)\" è¡¨ç¤ºè©²æ¡ˆä¾‹æ’é™¤äº† N å€‹å…§éƒ¨é›¢ç¾¤åˆ‡ç‰‡\n")
        f.write("- æ¯å€‹æ¡ˆä¾‹çš„è©³ç´°æ•¸æ“šï¼ˆåŒ…å«åŸå§‹çµ±è¨ˆå’Œé›¢ç¾¤åˆ‡ç‰‡è³‡è¨Šï¼‰ä¿å­˜åœ¨å°æ‡‰è³‡æ–™å¤¾çš„ `slices_data.json` ä¸­\n")