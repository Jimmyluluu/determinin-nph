#!/usr/bin/env python3
"""
Evans Index åˆ†æä¸»æ¨¡çµ„
"""
import os
import json
from typing import Dict, List, Optional
from utils import find_available_datasets, check_prelabeled_data_paths, load_hydrocephalus_reference, calculate_evans_index, validate_results_against_reference
from image_processing import create_brain_mask_from_original, merge_left_right_ventricles, find_frontal_horns_segment, find_skull_segment
from visualization import generate_evans_slice_screenshot, generate_markdown_report


def run_prelabeled_evans_analysis(base_path: str, dataset_name: str, occupancy_threshold: float = 0.7, generate_screenshots: bool = True, screenshot_output_dir: str = "evans_slices") -> Optional[Dict]:
    """
    ä½¿ç”¨æ¨™è¨˜å¥½çš„è³‡æ–™åŸ·è¡Œ Evans Index åˆ†æ

    Parameters:
        base_path (str): è³‡æ–™åŸºç¤è·¯å¾‘
        dataset_name (str): è³‡æ–™é›†åç¨±
        occupancy_threshold (float): ä½”æœ‰ç‡é–¾å€¼ï¼Œé è¨­0.7
        generate_screenshots (bool): æ˜¯å¦ç”Ÿæˆå¯è¦–åŒ–æˆªåœ–ï¼Œé è¨­True
        screenshot_output_dir (str): æˆªåœ–è¼¸å‡ºç›®éŒ„ï¼Œé è¨­"evans_slices"
    """

    # æª¢æŸ¥æª”æ¡ˆè·¯å¾‘
    paths, success = check_prelabeled_data_paths(base_path, dataset_name)
    if not success:
        return None

    # æº–å‚™è…¦å®¤é®ç½© - çµ±ä¸€ä½¿ç”¨å·¦å³è…¦å®¤åˆä½µ
    ventricle_mask_path = os.path.join(paths["dataset_path"], "merged_lateral_ventricles.nii.gz")
    if not os.path.exists(ventricle_mask_path):
        success = merge_left_right_ventricles(
            paths["ventricle_left"],
            paths["ventricle_right"],
            ventricle_mask_path,
            dataset_name,
            paths["original"]
        )
        if not success:
            return None

    # æº–å‚™è…¦éƒ¨é®ç½© - çµ±ä¸€ä½¿ç”¨åŸå§‹å½±åƒ
    brain_mask_path = os.path.join(paths["dataset_path"], "brain_mask_from_original.nii.gz")
    if not os.path.exists(brain_mask_path):
        success = create_brain_mask_from_original(paths["original"], brain_mask_path)
        if not success:
            print(f"âŒ ç„¡æ³•å»ºç«‹è…¦éƒ¨é®ç½©ï¼Œè·³é {dataset_name}")
            return None

    try:
        # æ‰¾å‡ºå´è…¦å®¤å‰è§’ä½ç½® - Evans Index æ¨™æº–æ¸¬é‡é»
        print("ğŸ” å°‹æ‰¾å´è…¦å®¤å‰è§’æ¸¬é‡æ®µ...")
        ventricle_segment = find_frontal_horns_segment(ventricle_mask_path, dataset_name, occupancy_threshold=occupancy_threshold)

        if ventricle_segment['width'] == 0:
            print(f"âŒ åœ¨ {dataset_name} ä¸­æ‰¾ä¸åˆ°æœ‰æ•ˆçš„è…¦å®¤æ®µ")
            return None

        # ä½¿ç”¨ç›¸åŒçš„ z, y åº§æ¨™æ‰¾å‡ºå°æ‡‰çš„é¡±éª¨æ®µ
        skull_segment = find_skull_segment(brain_mask_path, ventricle_segment['z'], ventricle_segment['y'])

        # è¨ˆç®— Evans Index
        evans_results = calculate_evans_index(
            ventricle_segment['width'],
            skull_segment['width']
        )

        # æ•´åˆçµæœ
        results = {
            "dataset": dataset_name,
            "ventricle_segment": ventricle_segment,
            "skull_segment": skull_segment,
            "evans_analysis": evans_results,
            "files_used": {
                "original": paths["original"],
                "ventricles": ventricle_mask_path,
                "brain_mask": brain_mask_path,
                "ventricle_left": paths.get("ventricle_left", "N/A"),
                "ventricle_right": paths.get("ventricle_right", "N/A"),
                "needs_merge": paths["needs_merge"]
            }
        }

        # ç”Ÿæˆå¯è¦–åŒ–æˆªåœ–
        if generate_screenshots:
            try:
                print(f"ğŸ–¼ï¸ æ­£åœ¨ç‚º {dataset_name} ç”Ÿæˆå¯è¦–åŒ–æˆªåœ–...")
                success = generate_evans_slice_screenshot(
                    case_name=dataset_name,
                    original_path=paths["original"],
                    ventricle_path=ventricle_mask_path,
                    brain_mask_path=brain_mask_path,
                    ventricle_coords=ventricle_segment,
                    skull_coords=skull_segment,
                    output_dir=screenshot_output_dir
                )

                if success:
                    screenshot_path = os.path.join(screenshot_output_dir, f'{dataset_name}_evans_slice.png')
                    results["screenshot_path"] = screenshot_path
                    print(f"âœ… æˆªåœ–å·²ç”Ÿæˆ: {screenshot_path}")
                else:
                    print(f"âš ï¸ {dataset_name}: æˆªåœ–ç”Ÿæˆå¤±æ•—")

            except Exception as screenshot_error:
                print(f"âŒ {dataset_name}: æˆªåœ–ç”Ÿæˆå‡ºéŒ¯ - {str(screenshot_error)}")

        return results

    except Exception as e:
        return None


def batch_analyze_prelabeled_data(base_path: str, datasets: Optional[List[str]] = None, occupancy_threshold: float = 0.7, generate_screenshots: bool = True, screenshot_output_dir: str = "evans_slices") -> Dict:
    """
    æ‰¹æ¬¡åˆ†æå¤šå€‹æ¨™è¨˜è³‡æ–™é›†

    Parameters:
        base_path (str): è³‡æ–™åŸºç¤è·¯å¾‘
        datasets (Optional[List[str]]): æŒ‡å®šè¦åˆ†æçš„è³‡æ–™é›†ï¼Œè‹¥ç‚ºNoneå‰‡åˆ†ææ‰€æœ‰å¯ç”¨è³‡æ–™é›†
        occupancy_threshold (float): ä½”æœ‰ç‡é–¾å€¼ï¼Œé è¨­0.7
        generate_screenshots (bool): æ˜¯å¦ç”Ÿæˆå¯è¦–åŒ–æˆªåœ–ï¼Œé è¨­True
        screenshot_output_dir (str): æˆªåœ–è¼¸å‡ºç›®éŒ„ï¼Œé è¨­"evans_slices"
    """
    if datasets is None:
        datasets = find_available_datasets(base_path)

    results = {}
    failed_cases = []
    successful = 0
    failed = 0

    for dataset in datasets:
        result = run_prelabeled_evans_analysis(base_path, dataset, occupancy_threshold, generate_screenshots, screenshot_output_dir)
        if result:
            results[dataset] = result
            successful += 1
        else:
            failed_cases.append(dataset)
            failed += 1

    print(f"\nåˆ†æå®Œæˆ: æˆåŠŸ {successful}, å¤±æ•— {failed}")
    if failed_cases:
        print(f"å¤±æ•—æ¡ˆä¾‹: {failed_cases}")

    # å°‡å¤±æ•—æ¡ˆä¾‹åŠ å…¥çµæœä¸­ä»¥ä¾¿å ±å‘Šé¡¯ç¤º
    results["_failed_cases"] = failed_cases

    # é¡¯ç¤ºçµ±è¨ˆæ‘˜è¦
    if results and len(results) > 1:  # ç¢ºä¿æœ‰å¯¦éš›çµæœï¼ˆä¸åªæ˜¯ _failed_casesï¼‰
        # æ’é™¤ç‰¹æ®Šéµ
        actual_results = {k: v for k, v in results.items() if not k.startswith('_')}
        if actual_results:
            import numpy as np
            evans_indices = [r["evans_analysis"]["evans_index"] for r in actual_results.values()]
            avg_evans = np.mean(evans_indices)
            high_risk_count = sum(1 for r in actual_results.values() if r["evans_analysis"]["hydrocephalus_risk"] == "é«˜")
            medium_risk_count = sum(1 for r in actual_results.values() if r["evans_analysis"]["hydrocephalus_risk"] == "ä¸­")

            print(f"\nğŸ“Š çµ±è¨ˆæ‘˜è¦:")
            print(f"   å¹³å‡ Evans Index: {avg_evans:.4f}")
            print(f"   è…¦å®¤æ“´å¤§æ¡ˆä¾‹: {high_risk_count}/{len(actual_results)} ({high_risk_count/len(actual_results)*100:.1f}%)")
            print(f"   å¯èƒ½/æ—©æœŸæ“´å¤§: {medium_risk_count}/{len(actual_results)} ({medium_risk_count/len(actual_results)*100:.1f}%)")

    return results


if __name__ == "__main__":
    # è¨­å®šæ¨™è¨˜è³‡æ–™çš„è·¯å¾‘
    LABELED_DATA_PATH = "/Volumes/Kuroé†¬ã®1TSSD/æ¨™è¨˜å¥½çš„è³‡æ–™"

    if not os.path.exists(LABELED_DATA_PATH):
        print(f"æ‰¾ä¸åˆ°è³‡æ–™è·¯å¾‘: {LABELED_DATA_PATH}")
        exit(1)

    # æ‰¾å‡ºæ‰€æœ‰å¯ç”¨çš„è³‡æ–™é›†
    available_datasets = find_available_datasets(LABELED_DATA_PATH)
    print(f"ç™¼ç¾ {len(available_datasets)} å€‹è³‡æ–™é›†")

    # è¨­å®šä½”æœ‰ç‡é–¾å€¼ - åªæœ‰ä½”æœ‰ç‡ >= æ­¤å€¼çš„è…¦å®¤æ®µæ‰æœƒè¢«è€ƒæ…®
    OCCUPANCY_THRESHOLD = 0.7  # å¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´æ­¤å€¼ (0.0-1.0)

    # è¨­å®šæˆªåœ–è¼¸å‡ºç›®éŒ„
    SCREENSHOT_OUTPUT_DIR = "evans_slices"
    os.makedirs(SCREENSHOT_OUTPUT_DIR, exist_ok=True)

    # åŸ·è¡Œæ‰¹æ¬¡åˆ†æï¼ˆåŒ…å«è‡ªå‹•æˆªåœ–ç”Ÿæˆï¼‰
    batch_results = batch_analyze_prelabeled_data(
        LABELED_DATA_PATH,
        occupancy_threshold=OCCUPANCY_THRESHOLD,
        generate_screenshots=True,
        screenshot_output_dir=SCREENSHOT_OUTPUT_DIR
    )

    # è¼‰å…¥å·²çŸ¥æ°´è…¦ç—‡åƒè€ƒæ¸…å–®
    known_hydrocephalus = load_hydrocephalus_reference()
    if known_hydrocephalus:
        print(f"\nè¼‰å…¥å·²çŸ¥æ°´è…¦ç—‡æ¡ˆä¾‹: {len(known_hydrocephalus)} å€‹")

        # é©—è­‰çµæœ
        validation = validate_results_against_reference(batch_results, known_hydrocephalus)

        print(f"\né©—è­‰çµæœ:")
        print(f"  ç¸½é«”æº–ç¢ºç‡: {validation['accuracy']:.1%}")
        print(f"  æ°´è…¦ç—‡æ­£ç¢ºè­˜åˆ¥: {validation['hydrocephalus_correctly_identified']}/{validation['known_hydrocephalus_count']}")
        print(f"  æ­£å¸¸æ¡ˆä¾‹æ­£ç¢ºè­˜åˆ¥: {validation['normal_correctly_identified']}")

        if validation['false_negatives']:
            print(f"  æ¼å ±æ¡ˆä¾‹: {len(validation['false_negatives'])} å€‹")

        if validation['false_positives']:
            print(f"  èª¤å ±æ¡ˆä¾‹: {len(validation['false_positives'])} å€‹")

        if validation['not_analyzed']:
            print(f"  æœªåˆ†ææ¡ˆä¾‹: {len(validation['not_analyzed'])} å€‹")

    # å»ºç«‹çµæœè³‡æ–™å¤¾
    result_dir = "result"
    os.makedirs(result_dir, exist_ok=True)

    # ä¿å­˜çµæœåˆ° JSON æª”æ¡ˆ
    output_file = os.path.join(result_dir, "prelabeled_evans_analysis_results.json")

    # å°‡é©—è­‰çµæœåŠ å…¥ JSON
    final_results = {
        "analysis_results": batch_results,
        "validation": validation if known_hydrocephalus else None,
        "known_hydrocephalus_cases": known_hydrocephalus
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)

    print(f"JSON çµæœå·²ä¿å­˜: {output_file}")

    # ç”¢ç”Ÿ Markdown å ±å‘Š
    md_file = os.path.join(result_dir, "prelabeled_evans_analysis_report.md")
    generate_markdown_report(batch_results, md_file, validation if known_hydrocephalus else None)
    print(f"Markdown å ±å‘Šå·²ä¿å­˜: {md_file}")

    print(f"\nğŸ“ æ‰€æœ‰çµæœæª”æ¡ˆå·²ä¿å­˜åœ¨ {result_dir}/ è³‡æ–™å¤¾")
    print(f"ğŸ–¼ï¸ å¯è¦–åŒ–æˆªåœ–å·²ä¿å­˜åœ¨ {SCREENSHOT_OUTPUT_DIR}/ è³‡æ–™å¤¾")