#!/usr/bin/env python3
"""
ç›´æ¥ä½¿ç”¨æ¨™è¨˜å¥½çš„è³‡æ–™é€²è¡Œ Evans Index åˆ†æ
"""
import os
import numpy as np
import nibabel as nib
import json
from typing import Dict, List, Optional
import glob

# å°å…¥å¯è¦–åŒ–æˆªåœ–ç”Ÿæˆå‡½æ•¸
from generate_evans_slices import generate_evans_slice_screenshot

def find_available_datasets(base_path: str) -> List[str]:
    """
    æ‰¾å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨™è¨˜è³‡æ–™é›†ï¼ˆåŒ…æ‹¬ data_X å’Œç—…ä¾‹è™Ÿæ ¼å¼ï¼‰
    """
    datasets = []

    # æ‰¾ data_X æ ¼å¼çš„è³‡æ–™å¤¾
    data_pattern = os.path.join(base_path, "data_*")
    for data_dir in glob.glob(data_pattern):
        if os.path.isdir(data_dir):
            dataset_name = os.path.basename(data_dir)
            if dataset_name not in ["data_16_not_ok"]:  # æ’é™¤å•é¡Œè³‡æ–™
                datasets.append(dataset_name)

    # æ‰¾ç—…ä¾‹è™Ÿæ ¼å¼çš„è³‡æ–™å¤¾ï¼ˆä»¥æ•¸å­—é–‹é ­çš„è³‡æ–™å¤¾ï¼‰
    case_pattern = os.path.join(base_path, "0*")
    for case_dir in glob.glob(case_pattern):
        if os.path.isdir(case_dir):
            case_name = os.path.basename(case_dir)
            datasets.append(case_name)

    datasets.sort()
    return datasets

def check_prelabeled_data_paths(base_path: str, dataset_name: str) -> Dict[str, str]:
    """
    æª¢æŸ¥æ¨™è¨˜è³‡æ–™çš„è·¯å¾‘æ˜¯å¦å­˜åœ¨ï¼ˆæ”¯æ´å…©ç¨®æ ¼å¼ï¼‰
    """
    dataset_path = os.path.join(base_path, dataset_name)

    # åˆ¤æ–·æ˜¯ data_X æ ¼å¼é‚„æ˜¯ç—…ä¾‹è™Ÿæ ¼å¼
    if dataset_name.startswith("data_"):
        # data_X æ ¼å¼
        dataset_num = dataset_name.split("_")[1]
        paths = {
            "dataset_path": dataset_path,
            "original": os.path.join(dataset_path, f"original_{dataset_num}.nii.gz"),
            "ventricles": os.path.join(dataset_path, f"mask_Ventricles_{dataset_num}.nii.gz"),
            "ventricle_left": os.path.join(dataset_path, f"mask_Ventricle_L_{dataset_num}.nii.gz"),
            "ventricle_right": os.path.join(dataset_path, f"mask_Ventricle_R_{dataset_num}.nii.gz"),
            "csf": os.path.join(dataset_path, f"mask_CSF_{dataset_num}.nii.gz"),
        }
    else:
        # ç—…ä¾‹è™Ÿæ ¼å¼
        paths = {
            "dataset_path": dataset_path,
            "original": os.path.join(dataset_path, "original.nii.gz"),
            "ventricles": os.path.join(dataset_path, "Ventricles.nii.gz"),
            "ventricle_left": os.path.join(dataset_path, "Ventricle_L.nii.gz"),
            "ventricle_right": os.path.join(dataset_path, "Ventricle_R.nii.gz"),
            "csf": os.path.join(dataset_path, "CSF.nii.gz"),
        }

    # æª¢æŸ¥å“ªäº›æª”æ¡ˆå­˜åœ¨
    existing_paths = {}
    missing_files = []

    for key, path in paths.items():
        if os.path.exists(path):
            existing_paths[key] = path
        else:
            missing_files.append(os.path.basename(path))

    # æª¢æŸ¥å¿…è¦æª”æ¡ˆ
    # Evans Index å¿…é ˆä½¿ç”¨å·¦å³å´è…¦å®¤ï¼Œä¸èƒ½ä½¿ç”¨åŒ…å«å››è…¦å®¤å’Œä¸‰è…¦å®¤çš„ Ventricles
    if "ventricle_left" in existing_paths and "ventricle_right" in existing_paths:
        existing_paths["needs_merge"] = True
    else:
        # æª¢æŸ¥æ˜¯å¦æœ‰ Ventricles æª”æ¡ˆä½†æ²’æœ‰å·¦å³åˆ†é›¢æª”æ¡ˆ
        if "ventricles" in existing_paths:
            print(f"âš ï¸ {dataset_name}: åªæœ‰ Ventricles æª”æ¡ˆï¼Œç„¡æ³•é€²è¡Œ Evans Index åˆ†æï¼ˆéœ€è¦å·¦å³è…¦å®¤åˆ†é›¢ï¼‰")
        else:
            print(f"âŒ {dataset_name}: ç¼ºå°‘å·¦å³è…¦å®¤æª”æ¡ˆ")
        return existing_paths, False

    # æª¢æŸ¥ original æª”æ¡ˆ
    if "original" not in existing_paths:
        print(f"âŒ {dataset_name}: ç¼ºå°‘åŸå§‹å½±åƒæª”æ¡ˆ")
        return existing_paths, False

    return existing_paths, True

def create_brain_mask_from_csf(csf_path: str, output_path: str, dilation_radius: int = 10) -> bool:
    """
    å¾ CSF é®ç½©å»ºç«‹è…¦éƒ¨å¤–è¼ªå»“é®ç½©
    """
    try:
        import SimpleITK as sitk

        # è®€å– CSF é®ç½©
        csf_img = sitk.ReadImage(csf_path)
        csf_binary = sitk.BinaryThreshold(csf_img, lowerThreshold=0.5, upperThreshold=10000,
                                         insideValue=1, outsideValue=0)

        # è†¨è„¹æ“ä½œä¾†å»ºç«‹è…¦éƒ¨å¤–è¼ªå»“
        dilate_filter = sitk.BinaryDilateImageFilter()
        dilate_filter.SetKernelRadius(dilation_radius)
        brain_mask = dilate_filter.Execute(csf_binary)

        # ä¿å­˜çµæœ
        sitk.WriteImage(brain_mask, output_path)
        print(f"âœ… å¾ CSF å»ºç«‹è…¦éƒ¨é®ç½©: {output_path}")
        return True

    except Exception as e:
        print(f"âŒ å»ºç«‹è…¦éƒ¨é®ç½©å¤±æ•—: {e}")
        return False

def create_brain_mask_from_original(original_path: str, output_path: str) -> bool:
    """
    å¾åŸå§‹å½±åƒå»ºç«‹é¡±å…§ç©ºé–“é®ç½©ï¼ˆç”¨æ–¼æ¸¬é‡é¡±éª¨å¯¬åº¦ï¼‰
    """
    try:
        import SimpleITK as sitk
        import numpy as np

        # è®€å–åŸå§‹å½±åƒ
        original_img = sitk.ReadImage(original_path, sitk.sitkFloat32)

        # è½‰æ›ç‚º numpy é™£åˆ—ä¾†è¨ˆç®—çµ±è¨ˆè³‡è¨Š
        img_array = sitk.GetArrayFromImage(original_img)

        # ç›´æ¥ä½¿ç”¨æ•´å€‹è…¦éƒ¨ï¼Œä¸åšé–¾å€¼è™•ç†

        # å»ºç«‹ç°¡å–®çš„éé›¶é®ç½©
        brain_mask = sitk.BinaryThreshold(original_img,
                                        lowerThreshold=1.0,  # ä»»ä½•éé›¶å€¼
                                        upperThreshold=100000,
                                        insideValue=1,
                                        outsideValue=0)

        # å½¢æ…‹å­¸æ“ä½œä¾†å»ºç«‹å®Œæ•´çš„é¡±å…§ç©ºé–“
        # 1. å…ˆå¡«å……æ‰€æœ‰å…§éƒ¨ç©ºæ´ï¼ˆåŒ…æ‹¬è…¦å®¤ç­‰ï¼‰
        fill_holes_filter = sitk.BinaryFillholeImageFilter()
        brain_mask = fill_holes_filter.Execute(brain_mask)

        # 2. é€²è¡Œé–‰é‹ç®—ä¾†é€£æ¥æ–·è£‚çš„å€åŸŸ
        closing_filter = sitk.BinaryMorphologicalClosingImageFilter()
        closing_filter.SetKernelRadius(5)
        brain_mask = closing_filter.Execute(brain_mask)

        # 3. å†æ¬¡å¡«å……ç©ºæ´ç¢ºä¿å®Œæ•´æ€§
        brain_mask = fill_holes_filter.Execute(brain_mask)

        # 4. è¼•å¾®è†¨è„¹ç¢ºä¿åŒ…å«å®Œæ•´é‚Šç•Œ
        dilate_filter = sitk.BinaryDilateImageFilter()
        dilate_filter.SetKernelRadius(2)
        brain_mask = dilate_filter.Execute(brain_mask)

        # ä¿å­˜çµæœ
        sitk.WriteImage(brain_mask, output_path)
        return True

    except Exception as e:
        return False

def merge_left_right_ventricles(left_path: str, right_path: str, output_path: str, dataset_name: str = "", original_path: str = None) -> bool:
    """
    åˆä½µå·¦å³è…¦å®¤é®ç½©ç‚ºå–®ä¸€æª”æ¡ˆï¼Œè™•ç†å°ºå¯¸ä¸åŒçš„æƒ…æ³
    """
    try:
        import SimpleITK as sitk
        import numpy as np
        import os

        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
        if not os.path.exists(left_path):
            print(f"âŒ {dataset_name}: å·¦è…¦å®¤æª”æ¡ˆä¸å­˜åœ¨: {left_path}")
            return False
        if not os.path.exists(right_path):
            print(f"âŒ {dataset_name}: å³è…¦å®¤æª”æ¡ˆä¸å­˜åœ¨: {right_path}")
            return False

        # æª¢æŸ¥æª”æ¡ˆå¤§å°
        left_size = os.path.getsize(left_path)
        right_size = os.path.getsize(right_path)
        if left_size == 0 or right_size == 0:
            print(f"âŒ {dataset_name}: æª”æ¡ˆå¤§å°ç‚º 0 (å·¦: {left_size}, å³: {right_size})")
            return False

        # è®€å–å·¦å³è…¦å®¤
        left_img = sitk.ReadImage(left_path)
        right_img = sitk.ReadImage(right_path)

        # æª¢æŸ¥å½±åƒå°ºå¯¸æ˜¯å¦ä¸€è‡´
        if left_img.GetSize() != right_img.GetSize():
            print(f"ğŸ” {dataset_name}: å·¦å³è…¦å®¤å½±åƒå°ºå¯¸ä¸ä¸€è‡´ (å·¦: {left_img.GetSize()}, å³: {right_img.GetSize()})")

            # å¦‚æœæœ‰åŸå§‹å½±åƒï¼Œä½¿ç”¨å®ƒä½œç‚ºåƒè€ƒç©ºé–“
            if original_path and os.path.exists(original_path):
                print(f"ğŸ”§ {dataset_name}: ä½¿ç”¨åŸå§‹å½±åƒä½œç‚ºåƒè€ƒç©ºé–“é€²è¡Œé‡æ–°æ¡æ¨£...")
                original_img = sitk.ReadImage(original_path)

                # é‡æ–°æ¡æ¨£å·¦å³è…¦å®¤åˆ°åŸå§‹å½±åƒç©ºé–“
                resampler = sitk.ResampleImageFilter()
                resampler.SetReferenceImage(original_img)
                resampler.SetInterpolator(sitk.sitkNearestNeighbor)  # ä½¿ç”¨æœ€è¿‘é„°æ’å€¼ä¿æŒäºŒé€²åˆ¶é®ç½©
                resampler.SetDefaultPixelValue(0)

                left_resampled = resampler.Execute(left_img)
                right_resampled = resampler.Execute(right_img)

                # è½‰æ›ç‚º numpy é€²è¡Œåˆä½µ
                left_array = sitk.GetArrayFromImage(left_resampled)
                right_array = sitk.GetArrayFromImage(right_resampled)

                # åˆä½µ
                merged_array = np.logical_or(left_array > 0.5, right_array > 0.5).astype(np.uint8)

                # è½‰å› SimpleITK å½±åƒ
                merged = sitk.GetImageFromArray(merged_array)
                merged.CopyInformation(original_img)

                print(f"âœ… {dataset_name}: é‡æ–°æ¡æ¨£å¾Œåˆä½µå®Œæˆ")
            else:
                print(f"âŒ {dataset_name}: ç„¡æ³•è™•ç†å°ºå¯¸ä¸ä¸€è‡´ä¸”ç¼ºå°‘åŸå§‹å½±åƒåƒè€ƒ")
                return False
        else:
            # å°ºå¯¸ä¸€è‡´çš„æ­£å¸¸è™•ç†
            left_array = sitk.GetArrayFromImage(left_img)
            right_array = sitk.GetArrayFromImage(right_img)

            # åˆä½µ
            merged_array = np.logical_or(left_array > 0.5, right_array > 0.5).astype(np.uint8)

            # è½‰å› SimpleITK å½±åƒ
            merged = sitk.GetImageFromArray(merged_array)
            merged.CopyInformation(left_img)

        # æª¢æŸ¥åˆä½µçµæœ
        merged_nonzero = np.count_nonzero(merged_array)
        if merged_nonzero == 0:
            print(f"âŒ {dataset_name}: åˆä½µå¾Œé®ç½©ç‚ºç©º")
            return False

        print(f"ğŸ” {dataset_name}: åˆä½µå¾Œéé›¶æ•¸ {merged_nonzero}")

        # ä¿å­˜çµæœ
        sitk.WriteImage(merged, output_path)
        print(f"âœ… {dataset_name}: åˆä½µå®Œæˆï¼Œä¿å­˜è‡³ {output_path}")
        return True

    except Exception as e:
        print(f"âŒ {dataset_name}: åˆä½µå¤±æ•— - {str(e)}")
        return False

def find_frontal_horns_segment(nii_path: str, dataset_name: str = "", max_reasonable_width: int = 200, occupancy_threshold: float = 0.7) -> Dict:
    """
    æ‰¾å‡ºå´è…¦å®¤å‰è§’çš„æ¸¬é‡æ®µ - æ ¹æ“šè³‡æ–™ä¾†æºé©æ‡‰ä¸åŒåº§æ¨™ç³»çµ±

    Parameters:
        nii_path (str): è…¦å®¤é®ç½©è·¯å¾‘
        dataset_name (str): è³‡æ–™é›†åç¨±ï¼Œç”¨æ–¼åˆ¤æ–·åº§æ¨™ç³»çµ±
        max_reasonable_width (int): æœ€å¤§åˆç†å¯¬åº¦ï¼Œé è¨­200
        occupancy_threshold (float): ä½”æœ‰ç‡é–¾å€¼ï¼Œé è¨­0.7
    """
    img = nib.load(nii_path)
    mask_data = img.get_fdata()
    binary = (mask_data > 0).astype(np.uint8)

    best = {'width': 0, 'z': None, 'y': None, 'x1': None, 'x2': None, 'occupancy': 0}
    X, Y, Z = binary.shape

    # æª¢æŸ¥é®ç½©æ˜¯å¦æœ‰å…§å®¹
    total_pixels = np.count_nonzero(binary)
    if total_pixels == 0:
        print(f"âŒ è…¦å®¤é®ç½©å®Œå…¨ç‚ºç©º")
        return best

    # æ‰¾å‡ºè…¦å®¤çš„ Z è»¸ç¯„åœ
    z_coords = []
    for z in range(Z):
        if np.count_nonzero(binary[:, :, z]) > 0:
            z_coords.append(z)

    if not z_coords:
        return best

    z_min, z_max = min(z_coords), max(z_coords)
    z_range = z_max - z_min

    # å‰è§’å€åŸŸï¼šZ è»¸å‰éƒ¨ (z/3 åˆ° z)
    if z_range > 0:
        target_z_start = int(z_min + z_range / 3)  # z/3
        target_z_end = z_max  # z
    else:
        target_z_start = target_z_end = z_min

    # æ ¹æ“šè³‡æ–™ä¾†æºæ±ºå®š Y è»¸æœç´¢ç¯„åœ
    y_mid = Y // 2  # Yè»¸ä¸­é»

    # data é–‹é ­çš„æª”æ¡ˆï¼šå‰è§’åœ¨ä¸‹åŠéƒ¨ (0 åˆ° n/2)
    # å…¶ä»–æª”æ¡ˆï¼šå‰è§’åœ¨ä¸ŠåŠéƒ¨ (n/2 åˆ° n)
    is_data_series = dataset_name.startswith('data_')

    if is_data_series:
        y_search_range = range(0, y_mid)
        search_description = "ä¸‹åŠéƒ¨ (data ç³»åˆ—)"
    else:
        y_search_range = range(y_mid, Y)
        search_description = "ä¸ŠåŠéƒ¨ (ç·¨è™Ÿç³»åˆ—)"

    for z in range(target_z_start, min(target_z_end + 1, Z)):
        slice_ = binary[:, :, z]

        if np.count_nonzero(slice_) == 0:
            continue

        # æ ¹æ“šè³‡æ–™é¡å‹æœç´¢å°æ‡‰çš„ Y ç¯„åœ
        for y in y_search_range:
            col = slice_[:, y]
            xs = np.where(col > 0)[0]

            if xs.size < 2:
                continue

            x1, x2 = xs.min(), xs.max()
            width = x2 - x1

            # æª¢æŸ¥å¯¬åº¦åˆç†æ€§
            if width > max_reasonable_width or width < 5:
                continue

            # æª¢æŸ¥ä½”æœ‰ç‡
            occupancy = col[x1:x2+1].sum() / (width + 1) if width > 0 else 0

            # åªæœ‰ä½”æœ‰ç‡é”åˆ°é–¾å€¼ä¸”å¯¬åº¦æ›´å¯¬æ™‚æ‰æ›´æ–°
            if occupancy >= occupancy_threshold and width > best['width']:
                best.update({
                    'width': int(width),
                    'z': int(z),
                    'y': int(y),
                    'x1': int(x1),
                    'x2': int(x2),
                    'occupancy': float(occupancy)
                })

    # èª¿è©¦è³‡è¨Š
    if best['width'] == 0:
        print(f"âŒ åœ¨å‰è§’å€åŸŸæ‰¾ä¸åˆ°æœ‰æ•ˆæ®µï¼ŒZç¯„åœ: {target_z_start}-{target_z_end}, Yç¯„åœ: {search_description}")
        # å›é€€åˆ°å…¨åŸŸæœå°‹æœ€å¯¬æ®µ
        return find_widest_segment_fallback(binary, max_reasonable_width, occupancy_threshold)
    else:
        print(f"âœ… æ‰¾åˆ°å‰è§’æ®µ: å¯¬åº¦={best['width']}, Z={best['z']}, Y={best['y']} ({search_description})")

    return best

def find_widest_segment_fallback(binary, max_reasonable_width, occupancy_threshold=0.7):
    """å›é€€æ–¹æ³•ï¼šæ‰¾æœ€å¯¬çš„è…¦å®¤æ®µ"""
    best = {'width': 0, 'z': None, 'y': None, 'x1': None, 'x2': None, 'occupancy': 0}
    X, Y, Z = binary.shape

    for z in range(Z):
        slice_ = binary[:, :, z]
        for y in range(Y):
            col = slice_[:, y]
            xs = np.where(col > 0)[0]
            if xs.size < 2:
                continue
            x1, x2 = xs.min(), xs.max()
            width = x2 - x1

            if width <= best['width'] or width > max_reasonable_width:
                continue

            occupancy = col[x1:x2+1].sum() / (width + 1)

            # åªæœ‰ä½”æœ‰ç‡é”åˆ°é–¾å€¼æ‰æ›´æ–°
            if occupancy >= occupancy_threshold:
                best.update({'width': int(width), 'z': int(z), 'y': int(y), 'x1': int(x1), 'x2': int(x2), 'occupancy': float(occupancy)})

    return best

def find_skull_segment(skull_path: str, z_fixed: int, y_ventricle: int, min_reasonable_width: int = 100) -> Dict:
    """
    åœ¨æŒ‡å®šçš„ Z åˆ‡ç‰‡ä¸Šæ‰¾åˆ°é¡±éª¨çš„æœ€å¤§å¯¬åº¦ï¼Œè€Œä¸æ˜¯å›ºå®šåœ¨è…¦å®¤çš„ Y åº§æ¨™
    """
    skull_img = nib.load(skull_path)
    skull_data = skull_img.get_fdata()
    skull_binary = (skull_data > 0).astype(np.uint8)

    slice_skull = skull_binary[:, :, z_fixed]

    # åœ¨æ•´å€‹ Z åˆ‡ç‰‡ä¸Šæ‰¾åˆ°é¡±éª¨çš„æœ€å¤§å¯¬åº¦
    max_width = 0
    best_y = y_ventricle
    best_x1, best_x2 = 0, 0
    best_occupancy = 0.0

    for y in range(slice_skull.shape[1]):
        col_skull = slice_skull[:, y]
        xs = np.where(col_skull > 0)[0]

        if xs.size >= 2:
            x1, x2 = xs.min(), xs.max()
            width = x2 - x1
            occupancy = col_skull[x1:x2+1].sum() / (width + 1)

            # æ›´æ–°æœ€å¤§å¯¬åº¦
            if width > max_width:
                max_width = width
                best_y = y
                best_x1, best_x2 = x1, x2
                best_occupancy = occupancy

    # æª¢æŸ¥æ˜¯å¦æ‰¾åˆ°åˆç†çš„é¡±éª¨å¯¬åº¦
    if max_width < min_reasonable_width:
        raise RuntimeError(f"åœ¨ z={z_fixed} æ‰¾åˆ°çš„æœ€å¤§é¡±éª¨å¯¬åº¦ {max_width} å°æ–¼æœ€å°åˆç†å€¼ {min_reasonable_width}")

    return {
        'width': int(max_width),
        'x1': int(best_x1),
        'x2': int(best_x2),
        'occupancy': float(best_occupancy),
        'z': int(z_fixed),
        'y': int(best_y)
    }

def calculate_evans_index(ventricle_width: float, skull_width: float) -> Dict:
    """
    è¨ˆç®— Evans Index ä¸¦æä¾›è‡¨åºŠè§£é‡‹
    """
    if skull_width == 0:
        return {"error": "é¡±éª¨å¯¬åº¦ä¸èƒ½ç‚ºé›¶"}

    evans_index = ventricle_width / skull_width

    # åˆç†æ€§æª¢æŸ¥
    warnings = []
    if evans_index > 1.0:
        warnings.append(f"ç•°å¸¸: Evans Index > 1.0 ({evans_index:.4f})")
    if ventricle_width > 300:
        warnings.append(f"ç•°å¸¸: è…¦å®¤å¯¬åº¦éå¤§ ({ventricle_width})")
    if skull_width < 100:
        warnings.append(f"ç•°å¸¸: é¡±éª¨å¯¬åº¦éå° ({skull_width})")

    # æ›´æ–°çš„è‡¨åºŠåˆ†é¡æ¨™æº–
    if evans_index <= 0.25:
        clinical_significance = "æ­£å¸¸ç¯„åœ (â‰¤ 0.25)"
        hydrocephalus_risk = "ä½"
    elif evans_index <= 0.30:
        clinical_significance = "å¯èƒ½æˆ–æ—©æœŸè…¦å®¤æ“´å¤§ (0.25-0.30)"
        hydrocephalus_risk = "ä¸­"
    else:
        clinical_significance = "è…¦å®¤æ“´å¤§ (> 0.30)"
        hydrocephalus_risk = "é«˜"

    result = {
        "evans_index": round(float(evans_index), 4),
        "ventricle_width": int(ventricle_width),
        "skull_width": int(skull_width),
        "clinical_significance": clinical_significance,
        "hydrocephalus_risk": hydrocephalus_risk,
        "warnings": warnings if warnings else None
    }

    return result

def run_prelabeled_evans_analysis(base_path: str, dataset_name: str, occupancy_threshold: float = 0.7, generate_screenshots: bool = True, screenshot_output_dir: str = "evans_slices") -> Optional[Dict]:
    """
    ä½¿ç”¨æ¨™è¨˜å¥½çš„è³‡æ–™åŸ·è¡Œ Evans Index åˆ†æ

    Parameters:
        base_path (str): è³‡æ–™åŸºç¤è·¯å¾‘
        dataset_name (str): è³‡æ–™é›†åç¨±
        occupancy_threshold (float): ä½”æœ‰ç‡é–¾å€¼ï¼Œé è¨­0.7
        generate_screenshots (bool): æ˜¯å¦ç”Ÿæˆå¯è¦–åŒ–æˆªåœ–ï¼Œé è¨­True
        screenshot_output_dir (str): æˆªåœ–è¼¸å‡ºç›®éŒ„ï¼Œé è¨­"evans_slices"
    """

    # æª¢æŸ¥æª”æ¡ˆè·¯å¾‘
    paths, success = check_prelabeled_data_paths(base_path, dataset_name)
    if not success:
        return None

    # æº–å‚™è…¦å®¤é®ç½© - çµ±ä¸€ä½¿ç”¨å·¦å³è…¦å®¤åˆä½µ
    ventricle_mask_path = os.path.join(paths["dataset_path"], "merged_lateral_ventricles.nii.gz")
    if not os.path.exists(ventricle_mask_path):
        success = merge_left_right_ventricles(
            paths["ventricle_left"],
            paths["ventricle_right"],
            ventricle_mask_path,
            dataset_name,
            paths["original"]
        )
        if not success:
            return None

    # æº–å‚™è…¦éƒ¨é®ç½© - çµ±ä¸€ä½¿ç”¨åŸå§‹å½±åƒ
    brain_mask_path = os.path.join(paths["dataset_path"], "brain_mask_from_original.nii.gz")
    if not os.path.exists(brain_mask_path):
        success = create_brain_mask_from_original(paths["original"], brain_mask_path)
        if not success:
            print(f"âŒ ç„¡æ³•å»ºç«‹è…¦éƒ¨é®ç½©ï¼Œè·³é {dataset_name}")
            return None

    try:
        # æ‰¾å‡ºå´è…¦å®¤å‰è§’ä½ç½® - Evans Index æ¨™æº–æ¸¬é‡é»
        print("ğŸ” å°‹æ‰¾å´è…¦å®¤å‰è§’æ¸¬é‡æ®µ...")
        ventricle_segment = find_frontal_horns_segment(ventricle_mask_path, dataset_name, occupancy_threshold=occupancy_threshold)

        if ventricle_segment['width'] == 0:
            print(f"âŒ åœ¨ {dataset_name} ä¸­æ‰¾ä¸åˆ°æœ‰æ•ˆçš„è…¦å®¤æ®µ")
            return None

        # ä½¿ç”¨ç›¸åŒçš„ z, y åº§æ¨™æ‰¾å‡ºå°æ‡‰çš„é¡±éª¨æ®µ
        skull_segment = find_skull_segment(brain_mask_path, ventricle_segment['z'], ventricle_segment['y'])

        # è¨ˆç®— Evans Index
        evans_results = calculate_evans_index(
            ventricle_segment['width'],
            skull_segment['width']
        )

        # æ•´åˆçµæœ
        results = {
            "dataset": dataset_name,
            "ventricle_segment": ventricle_segment,
            "skull_segment": skull_segment,
            "evans_analysis": evans_results,
            "files_used": {
                "original": paths["original"],
                "ventricles": ventricle_mask_path,
                "brain_mask": brain_mask_path,
                "ventricle_left": paths.get("ventricle_left", "N/A"),
                "ventricle_right": paths.get("ventricle_right", "N/A"),
                "needs_merge": paths["needs_merge"]
            }
        }

        # ç”Ÿæˆå¯è¦–åŒ–æˆªåœ–
        if generate_screenshots:
            try:
                print(f"ğŸ–¼ï¸ æ­£åœ¨ç‚º {dataset_name} ç”Ÿæˆå¯è¦–åŒ–æˆªåœ–...")
                success = generate_evans_slice_screenshot(
                    case_name=dataset_name,
                    original_path=paths["original"],
                    ventricle_path=ventricle_mask_path,
                    brain_mask_path=brain_mask_path,
                    ventricle_coords=ventricle_segment,
                    skull_coords=skull_segment,
                    output_dir=screenshot_output_dir
                )

                if success:
                    screenshot_path = os.path.join(screenshot_output_dir, f'{dataset_name}_evans_slice.png')
                    results["screenshot_path"] = screenshot_path
                    print(f"âœ… æˆªåœ–å·²ç”Ÿæˆ: {screenshot_path}")
                else:
                    print(f"âš ï¸ {dataset_name}: æˆªåœ–ç”Ÿæˆå¤±æ•—")

            except Exception as screenshot_error:
                print(f"âŒ {dataset_name}: æˆªåœ–ç”Ÿæˆå‡ºéŒ¯ - {str(screenshot_error)}")

        return results

    except Exception as e:
        return None

def batch_analyze_prelabeled_data(base_path: str, datasets: Optional[List[str]] = None, occupancy_threshold: float = 0.7, generate_screenshots: bool = True, screenshot_output_dir: str = "evans_slices") -> Dict:
    """
    æ‰¹æ¬¡åˆ†æå¤šå€‹æ¨™è¨˜è³‡æ–™é›†

    Parameters:
        base_path (str): è³‡æ–™åŸºç¤è·¯å¾‘
        datasets (Optional[List[str]]): æŒ‡å®šè¦åˆ†æçš„è³‡æ–™é›†ï¼Œè‹¥ç‚ºNoneå‰‡åˆ†ææ‰€æœ‰å¯ç”¨è³‡æ–™é›†
        occupancy_threshold (float): ä½”æœ‰ç‡é–¾å€¼ï¼Œé è¨­0.7
        generate_screenshots (bool): æ˜¯å¦ç”Ÿæˆå¯è¦–åŒ–æˆªåœ–ï¼Œé è¨­True
        screenshot_output_dir (str): æˆªåœ–è¼¸å‡ºç›®éŒ„ï¼Œé è¨­"evans_slices"
    """
    if datasets is None:
        datasets = find_available_datasets(base_path)


    results = {}
    failed_cases = []
    successful = 0
    failed = 0

    for dataset in datasets:
        result = run_prelabeled_evans_analysis(base_path, dataset, occupancy_threshold, generate_screenshots, screenshot_output_dir)
        if result:
            results[dataset] = result
            successful += 1
        else:
            failed_cases.append(dataset)
            failed += 1

    print(f"\nåˆ†æå®Œæˆ: æˆåŠŸ {successful}, å¤±æ•— {failed}")
    if failed_cases:
        print(f"å¤±æ•—æ¡ˆä¾‹: {failed_cases}")

    # å°‡å¤±æ•—æ¡ˆä¾‹åŠ å…¥çµæœä¸­ä»¥ä¾¿å ±å‘Šé¡¯ç¤º
    results["_failed_cases"] = failed_cases

    # é¡¯ç¤ºçµ±è¨ˆæ‘˜è¦
    if results and len(results) > 1:  # ç¢ºä¿æœ‰å¯¦éš›çµæœï¼ˆä¸åªæ˜¯ _failed_casesï¼‰
        # æ’é™¤ç‰¹æ®Šéµ
        actual_results = {k: v for k, v in results.items() if not k.startswith('_')}
        if actual_results:
            evans_indices = [r["evans_analysis"]["evans_index"] for r in actual_results.values()]
            avg_evans = np.mean(evans_indices)
            high_risk_count = sum(1 for r in actual_results.values() if r["evans_analysis"]["hydrocephalus_risk"] == "é«˜")
            medium_risk_count = sum(1 for r in actual_results.values() if r["evans_analysis"]["hydrocephalus_risk"] == "ä¸­")

            print(f"\nğŸ“Š çµ±è¨ˆæ‘˜è¦:")
            print(f"   å¹³å‡ Evans Index: {avg_evans:.4f}")
            print(f"   è…¦å®¤æ“´å¤§æ¡ˆä¾‹: {high_risk_count}/{len(actual_results)} ({high_risk_count/len(actual_results)*100:.1f}%)")
            print(f"   å¯èƒ½/æ—©æœŸæ“´å¤§: {medium_risk_count}/{len(actual_results)} ({medium_risk_count/len(actual_results)*100:.1f}%)")

    return results

def load_hydrocephalus_reference() -> List[str]:
    """
    è¼‰å…¥å·²çŸ¥æ°´è…¦ç—‡æ¡ˆä¾‹çš„åƒè€ƒæ¸…å–®
    """
    try:
        reference_file = "hydrocephalus_reference.json"
        if os.path.exists(reference_file):
            with open(reference_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get("hydrocephalus_cases", {}).get("cases", [])
        return []
    except Exception as e:
        return []

def validate_results_against_reference(results: Dict, known_hydrocephalus: List[str]) -> Dict:
    """
    é©—è­‰åˆ†æçµæœèˆ‡å·²çŸ¥è‡¨åºŠè¨ºæ–·çš„ä¸€è‡´æ€§
    """
    # æ’é™¤ç‰¹æ®Šéµä¾†è¨ˆç®—å¯¦éš›åˆ†æçš„æ¡ˆä¾‹æ•¸
    actual_results_count = len([k for k in results.keys() if not k.startswith('_')])

    validation = {
        "total_analyzed": actual_results_count,
        "known_hydrocephalus_count": len(known_hydrocephalus),
        "hydrocephalus_correctly_identified": 0,
        "normal_correctly_identified": 0,
        "false_negatives": [],  # æ‡‰è©²æ˜¯æ°´è…¦ç—‡ä½†è¢«åˆ¤ç‚ºæ­£å¸¸
        "false_positives": [],  # æ‡‰è©²æ˜¯æ­£å¸¸ä½†è¢«åˆ¤ç‚ºæ°´è…¦ç—‡
        "not_analyzed": [],
        "accuracy": 0.0
    }

    total_correct = 0

    # æª¢æŸ¥å·²çŸ¥æ°´è…¦ç—‡æ¡ˆä¾‹
    for case in known_hydrocephalus:
        if case in results:
            evans_index = results[case]["evans_analysis"]["evans_index"]
            if evans_index > 0.30:  # åªæœ‰ > 0.30 æ‰ç®—é æ¸¬ç‚ºç•°å¸¸
                validation["hydrocephalus_correctly_identified"] += 1
                total_correct += 1
            else:
                validation["false_negatives"].append({
                    "case": case,
                    "evans_index": evans_index
                })
        else:
            validation["not_analyzed"].append(case)

    # æª¢æŸ¥æ‡‰è©²æ˜¯æ­£å¸¸çš„æ¡ˆä¾‹
    for case, result in results.items():
        # è·³éç‰¹æ®Šéµ
        if case.startswith('_'):
            continue

        if case not in known_hydrocephalus:  # æ‡‰è©²æ˜¯æ­£å¸¸æ¡ˆä¾‹
            evans_index = result["evans_analysis"]["evans_index"]
            if evans_index <= 0.30:  # â‰¤ 0.30 æ‰ç®—é æ¸¬ç‚ºæ­£å¸¸
                validation["normal_correctly_identified"] += 1
                total_correct += 1
            else:
                validation["false_positives"].append({
                    "case": case,
                    "evans_index": evans_index
                })

    # è¨ˆç®—æº–ç¢ºç‡
    if validation["total_analyzed"] > 0:
        validation["accuracy"] = total_correct / validation["total_analyzed"]

    return validation

def generate_markdown_report(results: Dict, output_file: str, validation: Dict = None):
    """
    ç”¢ç”Ÿç°¡æ½”æ˜ç­çš„ Markdown å ±å‘Š
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# Evans Index åˆ†æå ±å‘Š\n\n")
        f.write(f"ğŸ“… åˆ†ææ™‚é–“: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        # åˆ†é›¢å¤±æ•—æ¡ˆä¾‹å’ŒæˆåŠŸçµæœ
        failed_cases = results.get("_failed_cases", [])
        actual_results = {k: v for k, v in results.items() if not k.startswith('_')}

        if not actual_results:
            f.write("âŒ æ²’æœ‰æˆåŠŸåˆ†æçš„è³‡æ–™é›†\n")
            if failed_cases:
                f.write(f"\n### âŒ åˆ†æå¤±æ•—æ¡ˆä¾‹ ({len(failed_cases)} å€‹)\n\n")
                for case in failed_cases:
                    f.write(f"- {case}\n")
            return

        # çµ±è¨ˆæ‘˜è¦
        evans_indices = [r["evans_analysis"]["evans_index"] for r in actual_results.values()]
        avg_evans = sum(evans_indices) / len(evans_indices)
        high_risk_count = sum(1 for r in actual_results.values() if r["evans_analysis"]["hydrocephalus_risk"] == "é«˜")
        medium_risk_count = sum(1 for r in actual_results.values() if r["evans_analysis"]["hydrocephalus_risk"] == "ä¸­")
        normal_count = sum(1 for r in actual_results.values() if r["evans_analysis"]["hydrocephalus_risk"] == "ä½")

        f.write("## ğŸ“Š çµ±è¨ˆæ‘˜è¦\n\n")
        f.write(f"- **ç¸½å…±åˆ†ææ¡ˆä¾‹**: {len(actual_results)} å€‹\n")
        if failed_cases:
            f.write(f"- **åˆ†æå¤±æ•—æ¡ˆä¾‹**: {len(failed_cases)} å€‹\n")
        f.write(f"- **å¹³å‡ Evans Index**: {avg_evans:.4f}\n")
        f.write(f"- **æ­£å¸¸ç¯„åœ (â‰¤ 0.25)**: {normal_count}/{len(actual_results)} ({normal_count/len(actual_results)*100:.1f}%)\n")
        f.write(f"- **å¯èƒ½/æ—©æœŸæ“´å¤§ (0.25-0.30)**: {medium_risk_count}/{len(actual_results)} ({medium_risk_count/len(actual_results)*100:.1f}%)\n")
        f.write(f"- **è…¦å®¤æ“´å¤§ (> 0.30)**: {high_risk_count}/{len(actual_results)} ({high_risk_count/len(actual_results)*100:.1f}%)\n\n")

        # åˆ†é¡çµ±è¨ˆ
        normal_cases = [k for k, v in actual_results.items() if v["evans_analysis"]["hydrocephalus_risk"] == "ä½"]
        medium_cases = [k for k, v in actual_results.items() if v["evans_analysis"]["hydrocephalus_risk"] == "ä¸­"]
        high_cases = [k for k, v in actual_results.items() if v["evans_analysis"]["hydrocephalus_risk"] == "é«˜"]

        f.write("## ğŸŸ¢ æ­£å¸¸ç¯„åœæ¡ˆä¾‹\n\n")
        if normal_cases:
            f.write("| æ¡ˆä¾‹ | Evans Index | è…¦å®¤å¯¬åº¦ | é¡±éª¨å¯¬åº¦ | æ¸¬é‡ä½ç½® (x,y,z) |\n")
            f.write("|------|-------------|----------|----------|------------------|\n")
            for case in sorted(normal_cases):
                r = actual_results[case]
                ei = r["evans_analysis"]["evans_index"]
                vw = r["evans_analysis"]["ventricle_width"]
                sw = r["evans_analysis"]["skull_width"]
                vs = r["ventricle_segment"]
                ss = r["skull_segment"]
                v_pos = f"({vs['x1']}-{vs['x2']},{vs['y']},{vs['z']})"
                s_pos = f"({ss['x1']}-{ss['x2']},{ss['y']},{ss['z']})"
                f.write(f"| {case} | {ei:.4f} | {vw} | {sw} | V:{v_pos} S:{s_pos} |\n")
        else:
            f.write("æ²’æœ‰æ­£å¸¸ç¯„åœçš„æ¡ˆä¾‹\n")

        f.write("\n## ğŸŸ¡ å¯èƒ½/æ—©æœŸæ“´å¤§æ¡ˆä¾‹\n\n")
        if medium_cases:
            f.write("| æ¡ˆä¾‹ | Evans Index | è…¦å®¤å¯¬åº¦ | é¡±éª¨å¯¬åº¦ | æ¸¬é‡ä½ç½® (x,y,z) | è‡¨åºŠæ„ç¾© |\n")
            f.write("|------|-------------|----------|----------|------------------|----------|\n")
            for case in sorted(medium_cases):
                r = actual_results[case]
                ei = r["evans_analysis"]["evans_index"]
                vw = r["evans_analysis"]["ventricle_width"]
                sw = r["evans_analysis"]["skull_width"]
                cs = r["evans_analysis"]["clinical_significance"]
                vs = r["ventricle_segment"]
                ss = r["skull_segment"]
                v_pos = f"({vs['x1']}-{vs['x2']},{vs['y']},{vs['z']})"
                s_pos = f"({ss['x1']}-{ss['x2']},{ss['y']},{ss['z']})"
                f.write(f"| {case} | {ei:.4f} | {vw} | {sw} | V:{v_pos} S:{s_pos} | {cs} |\n")
        else:
            f.write("æ²’æœ‰å¯èƒ½/æ—©æœŸæ“´å¤§æ¡ˆä¾‹\n")

        f.write("\n## ğŸ”´ è…¦å®¤æ“´å¤§æ¡ˆä¾‹\n\n")
        if high_cases:
            f.write("| æ¡ˆä¾‹ | Evans Index | è…¦å®¤å¯¬åº¦ | é¡±éª¨å¯¬åº¦ | æ¸¬é‡ä½ç½® (x,y,z) | è‡¨åºŠæ„ç¾© |\n")
            f.write("|------|-------------|----------|----------|------------------|----------|\n")
            for case in sorted(high_cases):
                r = actual_results[case]
                ei = r["evans_analysis"]["evans_index"]
                vw = r["evans_analysis"]["ventricle_width"]
                sw = r["evans_analysis"]["skull_width"]
                cs = r["evans_analysis"]["clinical_significance"]
                vs = r["ventricle_segment"]
                ss = r["skull_segment"]
                v_pos = f"({vs['x1']}-{vs['x2']},{vs['y']},{vs['z']})"
                s_pos = f"({ss['x1']}-{ss['x2']},{ss['y']},{ss['z']})"
                f.write(f"| {case} | {ei:.4f} | {vw} | {sw} | V:{v_pos} S:{s_pos} | {cs} |\n")
        else:
            f.write("æ²’æœ‰è…¦å®¤æ“´å¤§æ¡ˆä¾‹\n")

        # é©—è­‰çµæœï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        if validation:
            f.write("\n## ğŸ” é©—è­‰çµæœ\n\n")
            f.write(f"- **ç¸½é«”æº–ç¢ºç‡**: {validation['accuracy']:.1%}\n")
            f.write(f"- **æ°´è…¦ç—‡æ­£ç¢ºè­˜åˆ¥**: {validation['hydrocephalus_correctly_identified']}/{validation['known_hydrocephalus_count']}\n")
            f.write(f"- **æ­£å¸¸æ¡ˆä¾‹æ­£ç¢ºè­˜åˆ¥**: {validation['normal_correctly_identified']}\n")

            if validation['false_negatives']:
                f.write(f"- **æ¼å ±æ¡ˆä¾‹**: {len(validation['false_negatives'])} å€‹\n")

            if validation['false_positives']:
                f.write(f"- **èª¤å ±æ¡ˆä¾‹**: {len(validation['false_positives'])} å€‹\n")

            if validation['not_analyzed']:
                f.write(f"- **æœªåˆ†ææ¡ˆä¾‹**: {len(validation['not_analyzed'])} å€‹\n")

            # è©³ç´°é¡¯ç¤ºå·²çŸ¥æ°´è…¦ç—‡æ¡ˆä¾‹çš„é æ¸¬çµæœ
            f.write("\n### ğŸ“‹ å·²çŸ¥æ°´è…¦ç—‡æ¡ˆä¾‹é æ¸¬ç‹€æ³\n\n")
            f.write("| æ¡ˆä¾‹ | Evans Index | é æ¸¬çµæœ | å¯¦éš›ç‹€æ³ | ç‹€æ…‹ |\n")
            f.write("|------|-------------|----------|----------|------|\n")

            # ä½¿ç”¨é©—è­‰çµæœä¸­çš„å·²çŸ¥æ¡ˆä¾‹æ¸…å–®
            known_cases = [
                "000235496D", "000206288G", "000152785B",
                "000137208D", "000096384I", "000087554H"
            ]

            for case in known_cases:
                if case in results:
                    r = actual_results[case]
                    evans_index = r["evans_analysis"]["evans_index"]
                    predicted = r["evans_analysis"]["hydrocephalus_risk"]
                    status = "âœ… æ­£ç¢º" if evans_index > 0.30 else "âŒ æ¼å ±"
                    f.write(f"| {case} | {evans_index:.4f} | {predicted} é¢¨éšª | æœ‰æ°´è…¦ç—‡ | {status} |\n")
                else:
                    f.write(f"| {case} | - | æœªåˆ†æ | æœ‰æ°´è…¦ç—‡ | âŒ æœªåˆ†æ |\n")

        # èªªæ˜
        f.write("\n## ğŸ“– èªªæ˜\n\n")
        f.write("- **Evans Index**: è…¦å®¤å¯¬åº¦èˆ‡é¡±éª¨å¯¬åº¦çš„æ¯”å€¼\n")
        f.write("- **æ­£å¸¸ç¯„åœ**: â‰¤ 0.25\n")
        f.write("- **å¯èƒ½/æ—©æœŸè…¦å®¤æ“´å¤§**: 0.25-0.30\n")
        f.write("- **è…¦å®¤æ“´å¤§**: > 0.30\n")
        f.write("- **æ¸¬é‡æ–¹æ³•**: åœ¨ç›¸åŒ Z åˆ‡ç‰‡ä¸Šæ¸¬é‡è…¦å®¤å’Œé¡±éª¨çš„æœ€å¤§å¯¬åº¦\n\n")

        # å¤±æ•—æ¡ˆä¾‹æ¸…å–®ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        if failed_cases:
            f.write("## âŒ åˆ†æå¤±æ•—æ¡ˆä¾‹\n\n")
            f.write(f"ä»¥ä¸‹ {len(failed_cases)} å€‹æ¡ˆä¾‹åˆ†æå¤±æ•—:\n\n")
            for case in sorted(failed_cases):
                f.write(f"- {case}\n")
            f.write("\n**å¤±æ•—åŸå› å¯èƒ½åŒ…æ‹¬**: ç¼ºå°‘å¿…è¦æª”æ¡ˆã€è…¦å®¤é®ç½©å•é¡Œã€æˆ–æ¸¬é‡åƒæ•¸è¶…å‡ºåˆç†ç¯„åœ\n\n")

if __name__ == "__main__":
    # è¨­å®šæ¨™è¨˜è³‡æ–™çš„è·¯å¾‘
    LABELED_DATA_PATH = "/Volumes/Kuroé†¬ã®1TSSD/æ¨™è¨˜å¥½çš„è³‡æ–™"

    if not os.path.exists(LABELED_DATA_PATH):
        print(f"æ‰¾ä¸åˆ°è³‡æ–™è·¯å¾‘: {LABELED_DATA_PATH}")
        exit(1)

    # æ‰¾å‡ºæ‰€æœ‰å¯ç”¨çš„è³‡æ–™é›†
    available_datasets = find_available_datasets(LABELED_DATA_PATH)
    print(f"ç™¼ç¾ {len(available_datasets)} å€‹è³‡æ–™é›†")

    # è¨­å®šä½”æœ‰ç‡é–¾å€¼ - åªæœ‰ä½”æœ‰ç‡ >= æ­¤å€¼çš„è…¦å®¤æ®µæ‰æœƒè¢«è€ƒæ…®
    OCCUPANCY_THRESHOLD = 0.7  # å¯ä»¥æ ¹æ“šéœ€è¦èª¿æ•´æ­¤å€¼ (0.0-1.0)

    # è¨­å®šæˆªåœ–è¼¸å‡ºç›®éŒ„
    SCREENSHOT_OUTPUT_DIR = "evans_slices"
    os.makedirs(SCREENSHOT_OUTPUT_DIR, exist_ok=True)

    # åŸ·è¡Œæ‰¹æ¬¡åˆ†æï¼ˆåŒ…å«è‡ªå‹•æˆªåœ–ç”Ÿæˆï¼‰
    batch_results = batch_analyze_prelabeled_data(
        LABELED_DATA_PATH,
        occupancy_threshold=OCCUPANCY_THRESHOLD,
        generate_screenshots=True,
        screenshot_output_dir=SCREENSHOT_OUTPUT_DIR
    )

    # è¼‰å…¥å·²çŸ¥æ°´è…¦ç—‡åƒè€ƒæ¸…å–®
    known_hydrocephalus = load_hydrocephalus_reference()
    if known_hydrocephalus:
        print(f"\nè¼‰å…¥å·²çŸ¥æ°´è…¦ç—‡æ¡ˆä¾‹: {len(known_hydrocephalus)} å€‹")

        # é©—è­‰çµæœ
        validation = validate_results_against_reference(batch_results, known_hydrocephalus)

        print(f"\né©—è­‰çµæœ:")
        print(f"  ç¸½é«”æº–ç¢ºç‡: {validation['accuracy']:.1%}")
        print(f"  æ°´è…¦ç—‡æ­£ç¢ºè­˜åˆ¥: {validation['hydrocephalus_correctly_identified']}/{validation['known_hydrocephalus_count']}")
        print(f"  æ­£å¸¸æ¡ˆä¾‹æ­£ç¢ºè­˜åˆ¥: {validation['normal_correctly_identified']}")

        if validation['false_negatives']:
            print(f"  æ¼å ±æ¡ˆä¾‹: {len(validation['false_negatives'])} å€‹")

        if validation['false_positives']:
            print(f"  èª¤å ±æ¡ˆä¾‹: {len(validation['false_positives'])} å€‹")

        if validation['not_analyzed']:
            print(f"  æœªåˆ†ææ¡ˆä¾‹: {len(validation['not_analyzed'])} å€‹")

    # å»ºç«‹çµæœè³‡æ–™å¤¾
    result_dir = "result"
    os.makedirs(result_dir, exist_ok=True)

    # ä¿å­˜çµæœåˆ° JSON æª”æ¡ˆ
    output_file = os.path.join(result_dir, "prelabeled_evans_analysis_results.json")

    # å°‡é©—è­‰çµæœåŠ å…¥ JSON
    final_results = {
        "analysis_results": batch_results,
        "validation": validation if known_hydrocephalus else None,
        "known_hydrocephalus_cases": known_hydrocephalus
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)

    print(f"JSON çµæœå·²ä¿å­˜: {output_file}")

    # ç”¢ç”Ÿ Markdown å ±å‘Š
    md_file = os.path.join(result_dir, "prelabeled_evans_analysis_report.md")
    generate_markdown_report(batch_results, md_file, validation if known_hydrocephalus else None)
    print(f"Markdown å ±å‘Šå·²ä¿å­˜: {md_file}")

    print(f"\nğŸ“ æ‰€æœ‰çµæœæª”æ¡ˆå·²ä¿å­˜åœ¨ {result_dir}/ è³‡æ–™å¤¾")
    print(f"ğŸ–¼ï¸ å¯è¦–åŒ–æˆªåœ–å·²ä¿å­˜åœ¨ {SCREENSHOT_OUTPUT_DIR}/ è³‡æ–™å¤¾")
