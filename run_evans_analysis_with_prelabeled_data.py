#!/usr/bin/env python3
"""
ç›´æ¥ä½¿ç”¨æ¨™è¨˜å¥½çš„è³‡æ–™é€²è¡Œ Evans Index åˆ†æ
"""
import os
import numpy as np
import nibabel as nib
import json
from typing import Dict, List, Optional
import glob

def find_available_datasets(base_path: str) -> List[str]:
    """
    æ‰¾å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨™è¨˜è³‡æ–™é›†ï¼ˆåŒ…æ‹¬ data_X å’Œç—…ä¾‹è™Ÿæ ¼å¼ï¼‰
    """
    datasets = []

    # æ‰¾ data_X æ ¼å¼çš„è³‡æ–™å¤¾
    data_pattern = os.path.join(base_path, "data_*")
    for data_dir in glob.glob(data_pattern):
        if os.path.isdir(data_dir):
            dataset_name = os.path.basename(data_dir)
            if dataset_name not in ["data_16_not_ok"]:  # æ’é™¤å•é¡Œè³‡æ–™
                datasets.append(dataset_name)

    # æ‰¾ç—…ä¾‹è™Ÿæ ¼å¼çš„è³‡æ–™å¤¾ï¼ˆä»¥æ•¸å­—é–‹é ­çš„è³‡æ–™å¤¾ï¼‰
    case_pattern = os.path.join(base_path, "0*")
    for case_dir in glob.glob(case_pattern):
        if os.path.isdir(case_dir):
            case_name = os.path.basename(case_dir)
            datasets.append(case_name)

    datasets.sort()
    return datasets

def check_prelabeled_data_paths(base_path: str, dataset_name: str) -> Dict[str, str]:
    """
    æª¢æŸ¥æ¨™è¨˜è³‡æ–™çš„è·¯å¾‘æ˜¯å¦å­˜åœ¨ï¼ˆæ”¯æ´å…©ç¨®æ ¼å¼ï¼‰
    """
    dataset_path = os.path.join(base_path, dataset_name)

    # åˆ¤æ–·æ˜¯ data_X æ ¼å¼é‚„æ˜¯ç—…ä¾‹è™Ÿæ ¼å¼
    if dataset_name.startswith("data_"):
        # data_X æ ¼å¼
        dataset_num = dataset_name.split("_")[1]
        paths = {
            "dataset_path": dataset_path,
            "original": os.path.join(dataset_path, f"original_{dataset_num}.nii.gz"),
            "ventricles": os.path.join(dataset_path, f"mask_Ventricles_{dataset_num}.nii.gz"),
            "ventricle_left": os.path.join(dataset_path, f"mask_Ventricle_L_{dataset_num}.nii.gz"),
            "ventricle_right": os.path.join(dataset_path, f"mask_Ventricle_R_{dataset_num}.nii.gz"),
            "csf": os.path.join(dataset_path, f"mask_CSF_{dataset_num}.nii.gz"),
        }
    else:
        # ç—…ä¾‹è™Ÿæ ¼å¼
        paths = {
            "dataset_path": dataset_path,
            "original": os.path.join(dataset_path, "original.nii.gz"),
            "ventricles": os.path.join(dataset_path, "Ventricles.nii.gz"),
            "ventricle_left": os.path.join(dataset_path, "Ventricle_L.nii.gz"),
            "ventricle_right": os.path.join(dataset_path, "Ventricle_R.nii.gz"),
            "csf": os.path.join(dataset_path, "CSF.nii.gz"),
        }

    # æª¢æŸ¥å“ªäº›æª”æ¡ˆå­˜åœ¨
    existing_paths = {}
    missing_files = []

    for key, path in paths.items():
        if os.path.exists(path):
            existing_paths[key] = path
        else:
            missing_files.append(os.path.basename(path))

    # æª¢æŸ¥å¿…è¦æª”æ¡ˆ
    # å¦‚æœæ²’æœ‰åˆä½µçš„ ventriclesï¼Œæª¢æŸ¥æ˜¯å¦æœ‰å·¦å³è…¦å®¤æª”æ¡ˆ
    if "ventricles" not in existing_paths:
        if "ventricle_left" in existing_paths and "ventricle_right" in existing_paths:
            # éœ€è¦åˆä½µå·¦å³è…¦å®¤
            print(f"â„¹ï¸ {dataset_name}: æ²’æœ‰åˆä½µè…¦å®¤æª”æ¡ˆï¼Œå°‡åˆä½µå·¦å³è…¦å®¤")
            existing_paths["needs_merge"] = True
        else:
            print(f"âŒ {dataset_name}: ç¼ºå°‘è…¦å®¤æª”æ¡ˆï¼ˆåˆä½µæˆ–å·¦å³åˆ†åˆ¥ï¼‰")
            return existing_paths, False
    else:
        existing_paths["needs_merge"] = False

    # æª¢æŸ¥ original æª”æ¡ˆ
    if "original" not in existing_paths:
        print(f"âŒ {dataset_name}: ç¼ºå°‘åŸå§‹å½±åƒæª”æ¡ˆ")
        return existing_paths, False

    print(f"âœ… {dataset_name}: æª”æ¡ˆæª¢æŸ¥é€šé")
    return existing_paths, True

def create_brain_mask_from_csf(csf_path: str, output_path: str, dilation_radius: int = 10) -> bool:
    """
    å¾ CSF é®ç½©å»ºç«‹è…¦éƒ¨å¤–è¼ªå»“é®ç½©
    """
    try:
        import SimpleITK as sitk

        # è®€å– CSF é®ç½©
        csf_img = sitk.ReadImage(csf_path)
        csf_binary = sitk.BinaryThreshold(csf_img, lowerThreshold=0.5, upperThreshold=10000,
                                         insideValue=1, outsideValue=0)

        # è†¨è„¹æ“ä½œä¾†å»ºç«‹è…¦éƒ¨å¤–è¼ªå»“
        dilate_filter = sitk.BinaryDilateImageFilter()
        dilate_filter.SetKernelRadius(dilation_radius)
        brain_mask = dilate_filter.Execute(csf_binary)

        # ä¿å­˜çµæœ
        sitk.WriteImage(brain_mask, output_path)
        print(f"âœ… å¾ CSF å»ºç«‹è…¦éƒ¨é®ç½©: {output_path}")
        return True

    except Exception as e:
        print(f"âŒ å»ºç«‹è…¦éƒ¨é®ç½©å¤±æ•—: {e}")
        return False

def create_brain_mask_from_original(original_path: str, output_path: str) -> bool:
    """
    å¾åŸå§‹å½±åƒå»ºç«‹é¡±å…§ç©ºé–“é®ç½©ï¼ˆç”¨æ–¼æ¸¬é‡é¡±éª¨å¯¬åº¦ï¼‰
    """
    try:
        import SimpleITK as sitk
        import numpy as np

        # è®€å–åŸå§‹å½±åƒ
        original_img = sitk.ReadImage(original_path, sitk.sitkFloat32)

        # è½‰æ›ç‚º numpy é™£åˆ—ä¾†è¨ˆç®—çµ±è¨ˆè³‡è¨Š
        img_array = sitk.GetArrayFromImage(original_img)

        # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
        non_zero_values = img_array[img_array > 0]
        if len(non_zero_values) == 0:
            print(f"âŒ å½±åƒä¸­æ²’æœ‰éé›¶å€¼")
            return False

        mean_intensity = np.mean(non_zero_values)
        std_intensity = np.std(non_zero_values)

        # ä½¿ç”¨éå¸¸ä½çš„é–¾å€¼ä¾†åŒ…å«æ•´å€‹é¡±å…§ç©ºé–“ï¼ˆåŒ…æ‹¬ CSFã€ç°è³ªã€ç™½è³ªï¼‰
        adaptive_threshold = max(1.0, mean_intensity * 0.02)  # é™åˆ° 2%

        print(f"ğŸ“Š å½±åƒçµ±è¨ˆ - å¹³å‡: {mean_intensity:.2f}, æ¨™æº–å·®: {std_intensity:.2f}, é–¾å€¼: {adaptive_threshold:.2f}")

        # ä½¿ç”¨ä½é–¾å€¼åˆ†å‰²ä¾†å»ºç«‹é¡±å…§ç©ºé–“é®ç½©
        brain_mask = sitk.BinaryThreshold(original_img,
                                        lowerThreshold=adaptive_threshold,
                                        upperThreshold=100000,
                                        insideValue=1,
                                        outsideValue=0)

        # å½¢æ…‹å­¸æ“ä½œä¾†å»ºç«‹å®Œæ•´çš„é¡±å…§ç©ºé–“
        # 1. å…ˆå¡«å……æ‰€æœ‰å…§éƒ¨ç©ºæ´ï¼ˆåŒ…æ‹¬è…¦å®¤ç­‰ï¼‰
        fill_holes_filter = sitk.BinaryFillholeImageFilter()
        brain_mask = fill_holes_filter.Execute(brain_mask)

        # 2. é€²è¡Œé–‰é‹ç®—ä¾†é€£æ¥æ–·è£‚çš„å€åŸŸ
        closing_filter = sitk.BinaryMorphologicalClosingImageFilter()
        closing_filter.SetKernelRadius(5)
        brain_mask = closing_filter.Execute(brain_mask)

        # 3. å†æ¬¡å¡«å……ç©ºæ´ç¢ºä¿å®Œæ•´æ€§
        brain_mask = fill_holes_filter.Execute(brain_mask)

        # 4. è¼•å¾®è†¨è„¹ç¢ºä¿åŒ…å«å®Œæ•´é‚Šç•Œ
        dilate_filter = sitk.BinaryDilateImageFilter()
        dilate_filter.SetKernelRadius(2)
        brain_mask = dilate_filter.Execute(brain_mask)

        # æª¢æŸ¥çµæœ
        result_array = sitk.GetArrayFromImage(brain_mask)
        print(f"ğŸ” é¡±å…§é®ç½©çµ±è¨ˆ: éé›¶åƒç´  {np.count_nonzero(result_array)}")

        # ä¿å­˜çµæœ
        sitk.WriteImage(brain_mask, output_path)
        print(f"âœ… å¾åŸå§‹å½±åƒå»ºç«‹é¡±å…§ç©ºé–“é®ç½©: {output_path}")
        return True

    except Exception as e:
        print(f"âŒ å¾åŸå§‹å½±åƒå»ºç«‹é¡±å…§é®ç½©å¤±æ•—: {e}")
        return False

def merge_left_right_ventricles(left_path: str, right_path: str, output_path: str) -> bool:
    """
    åˆä½µå·¦å³è…¦å®¤é®ç½©ç‚ºå–®ä¸€æª”æ¡ˆ
    """
    try:
        import SimpleITK as sitk
        import numpy as np

        # è®€å–å·¦å³è…¦å®¤
        left_img = sitk.ReadImage(left_path)
        right_img = sitk.ReadImage(right_path)

        # æª¢æŸ¥å½±åƒå°ºå¯¸æ˜¯å¦ä¸€è‡´
        if left_img.GetSize() != right_img.GetSize():
            print(f"âŒ å·¦å³è…¦å®¤å½±åƒå°ºå¯¸ä¸ä¸€è‡´: {left_img.GetSize()} vs {right_img.GetSize()}")
            return False

        # è½‰æ›ç‚º numpy é™£åˆ—æª¢æŸ¥è³‡æ–™
        left_array = sitk.GetArrayFromImage(left_img)
        right_array = sitk.GetArrayFromImage(right_img)

        print(f"ğŸ” å·¦è…¦å®¤çµ±è¨ˆ: ç¯„åœ[{left_array.min():.2f}, {left_array.max():.2f}], éé›¶åƒç´ : {np.count_nonzero(left_array)}")
        print(f"ğŸ” å³è…¦å®¤çµ±è¨ˆ: ç¯„åœ[{right_array.min():.2f}, {right_array.max():.2f}], éé›¶åƒç´ : {np.count_nonzero(right_array)}")

        # äºŒå€¼åŒ–
        left_binary = sitk.BinaryThreshold(left_img, lowerThreshold=0.5, upperThreshold=10000,
                                         insideValue=1, outsideValue=0)
        right_binary = sitk.BinaryThreshold(right_img, lowerThreshold=0.5, upperThreshold=10000,
                                          insideValue=1, outsideValue=0)

        # åˆä½µï¼ˆé‚è¼¯ ORï¼‰
        merged = sitk.Or(left_binary, right_binary)

        # æª¢æŸ¥åˆä½µçµæœ
        merged_array = sitk.GetArrayFromImage(merged)
        print(f"ğŸ” åˆä½µçµæœ: éé›¶åƒç´ : {np.count_nonzero(merged_array)}")

        # ä¿å­˜çµæœ
        sitk.WriteImage(merged, output_path)
        print(f"âœ… åˆä½µå·¦å³è…¦å®¤: {output_path}")
        return True

    except Exception as e:
        print(f"âŒ åˆä½µè…¦å®¤å¤±æ•—: {e}")
        return False

def find_best_ventricle_segment(nii_path: str, occupancy_threshold: float = 0.8, max_reasonable_width: int = 200) -> Dict:
    """
    æ‰¾å‡ºè…¦å®¤çš„æœ€ä½³æ¸¬é‡æ®µï¼ˆå¾ç¾æœ‰çš„ notebook ç¨‹å¼ç¢¼è¤‡è£½ï¼‰
    """
    img = nib.load(nii_path)
    mask_data = img.get_fdata()
    binary = (mask_data > 0).astype(np.uint8)

    best = {'width': 0, 'z': None, 'y': None, 'x1': None, 'x2': None, 'occupancy': 0}
    X, Y, Z = binary.shape

    print(f"ğŸ” è…¦å®¤é®ç½©å°ºå¯¸: {binary.shape}, éé›¶åƒç´ : {np.count_nonzero(binary)}")

    suspicious_segments = []

    for z in range(Z):
        slice_ = binary[:, :, z]
        for y in range(Y):
            col = slice_[:, y]
            xs = np.where(col > 0)[0]
            if xs.size < 2:
                continue
            x1, x2 = xs.min(), xs.max()
            width = x2 - x1
            if width <= best['width']:
                continue

            # æª¢æŸ¥å¯¬åº¦æ˜¯å¦åˆç†
            if width > max_reasonable_width:
                suspicious_segments.append({
                    'width': width, 'z': z, 'y': y, 'x1': x1, 'x2': x2
                })
                print(f"âš ï¸ è·³éç•°å¸¸å¯¬åº¦çš„è…¦å®¤æ®µ: {width} > {max_reasonable_width} at z={z}, y={y}, x1={x1}, x2={x2}")
                continue

            occupancy = col[x1:x2+1].sum() / (width + 1)
            if occupancy >= occupancy_threshold:
                print(f"ğŸ“ æ‰¾åˆ°å€™é¸æ®µ: å¯¬åº¦={width}, z={z}, y={y}, ä½”æœ‰ç‡={occupancy:.3f}")
                best.update({'width': int(width), 'z': int(z), 'y': int(y), 'x1': int(x1), 'x2': int(x2), 'occupancy': float(occupancy)})

    if suspicious_segments:
        print(f"âš ï¸ ç¸½å…±è·³é {len(suspicious_segments)} å€‹ç•°å¸¸å¯¬åº¦çš„æ®µ")

    if best['width'] == 0:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è…¦å®¤æ®µ")

    return best

def find_skull_segment(skull_path: str, z_fixed: int, min_reasonable_width: int = 100) -> Dict:
    """
    æ‰¾å‡ºé¡±éª¨çš„æœ€å¯¬æ®µï¼ˆå¾ç¾æœ‰çš„ notebook ç¨‹å¼ç¢¼è¤‡è£½ï¼‰
    """
    skull_img = nib.load(skull_path)
    skull_data = skull_img.get_fdata()
    skull_binary = (skull_data > 0).astype(np.uint8)

    slice_skull = skull_binary[:, :, z_fixed]
    best_segment = None

    for y in range(slice_skull.shape[1]):
        col_skull = slice_skull[:, y]
        xs = np.where(col_skull > 0)[0]

        if xs.size < 2:
            continue

        x1_skull, x2_skull = xs.min(), xs.max()
        width_skull = x2_skull - x1_skull
        occupancy_skull = col_skull[x1_skull:x2_skull+1].sum() / (width_skull + 1)

        if (best_segment is None) or (width_skull > best_segment['width']):
            # æª¢æŸ¥é¡±éª¨å¯¬åº¦æ˜¯å¦åˆç†
            if width_skull < min_reasonable_width:
                print(f"âš ï¸ è·³éç•°å¸¸å°çš„é¡±éª¨æ®µ: {width_skull} < {min_reasonable_width} at z={z_fixed}, y={y}")
                continue
            best_segment = {
                'width': int(width_skull),
                'x1': int(x1_skull),
                'x2': int(x2_skull),
                'occupancy': float(occupancy_skull),
                'z': int(z_fixed),
                'y': int(y)
            }

    if best_segment is None:
        raise RuntimeError(f"åœ¨ z={z_fixed} æ‰¾ä¸åˆ°é¡±éª¨æ®µ")

    return best_segment

def calculate_evans_index(ventricle_width: float, skull_width: float) -> Dict:
    """
    è¨ˆç®— Evans Index ä¸¦æä¾›è‡¨åºŠè§£é‡‹
    """
    if skull_width == 0:
        return {"error": "é¡±éª¨å¯¬åº¦ä¸èƒ½ç‚ºé›¶"}

    evans_index = ventricle_width / skull_width

    # åˆç†æ€§æª¢æŸ¥
    warnings = []
    if evans_index > 1.0:
        warnings.append(f"ç•°å¸¸: Evans Index > 1.0 ({evans_index:.4f})")
    if ventricle_width > 300:
        warnings.append(f"ç•°å¸¸: è…¦å®¤å¯¬åº¦éå¤§ ({ventricle_width})")
    if skull_width < 100:
        warnings.append(f"ç•°å¸¸: é¡±éª¨å¯¬åº¦éå° ({skull_width})")

    result = {
        "evans_index": round(float(evans_index), 4),
        "ventricle_width": int(ventricle_width),
        "skull_width": int(skull_width),
        "clinical_significance": "å¯èƒ½æœ‰è…¦æ°´è…« (æŒ‡æ•¸ > 0.3)" if evans_index > 0.3 else "æ­£å¸¸ç¯„åœ (â‰¤ 0.3)",
        "hydrocephalus_risk": "é«˜" if evans_index > 0.3 else "ä½",
        "warnings": warnings if warnings else None
    }

    return result

def run_prelabeled_evans_analysis(base_path: str, dataset_name: str) -> Optional[Dict]:
    """
    ä½¿ç”¨æ¨™è¨˜å¥½çš„è³‡æ–™åŸ·è¡Œ Evans Index åˆ†æ
    """
    print(f"\nğŸ” é–‹å§‹åˆ†æ: {dataset_name}")

    # æª¢æŸ¥æª”æ¡ˆè·¯å¾‘
    paths, success = check_prelabeled_data_paths(base_path, dataset_name)
    if not success:
        return None

    # æº–å‚™è…¦å®¤é®ç½©
    ventricle_mask_path = None
    if paths["needs_merge"]:
        # éœ€è¦åˆä½µå·¦å³è…¦å®¤
        ventricle_mask_path = os.path.join(paths["dataset_path"], "merged_ventricles.nii.gz")
        if not os.path.exists(ventricle_mask_path):
            success = merge_left_right_ventricles(
                paths["ventricle_left"],
                paths["ventricle_right"],
                ventricle_mask_path
            )
            if not success:
                print(f"âŒ ç„¡æ³•åˆä½µè…¦å®¤é®ç½©ï¼Œè·³é {dataset_name}")
                return None
    else:
        # ä½¿ç”¨ç¾æœ‰çš„åˆä½µè…¦å®¤æª”æ¡ˆ
        ventricle_mask_path = paths["ventricles"]

    # æº–å‚™è…¦éƒ¨é®ç½© - çµ±ä¸€ä½¿ç”¨åŸå§‹å½±åƒ
    print(f"ğŸ§  ä½¿ç”¨åŸå§‹å½±åƒå»ºç«‹è…¦éƒ¨é®ç½©...")
    brain_mask_path = os.path.join(paths["dataset_path"], "brain_mask_from_original.nii.gz")
    if not os.path.exists(brain_mask_path):
        success = create_brain_mask_from_original(paths["original"], brain_mask_path)
        if not success:
            print(f"âŒ ç„¡æ³•å»ºç«‹è…¦éƒ¨é®ç½©ï¼Œè·³é {dataset_name}")
            return None

    try:
        # æ‰¾å‡ºæœ€ä½³è…¦å®¤æ®µ
        print("ğŸ” å°‹æ‰¾æœ€ä½³è…¦å®¤æ¸¬é‡æ®µ...")
        ventricle_segment = find_best_ventricle_segment(ventricle_mask_path)

        if ventricle_segment['width'] == 0:
            print(f"âŒ åœ¨ {dataset_name} ä¸­æ‰¾ä¸åˆ°æœ‰æ•ˆçš„è…¦å®¤æ®µ")
            return None

        # ä½¿ç”¨ç›¸åŒçš„ z åˆ‡ç‰‡æ‰¾å‡ºé¡±éª¨æ®µ
        print("ğŸ” å°‹æ‰¾å°æ‡‰çš„é¡±éª¨æ¸¬é‡æ®µ...")
        skull_segment = find_skull_segment(brain_mask_path, ventricle_segment['z'])

        # è¨ˆç®— Evans Index
        print("ğŸ§® è¨ˆç®— Evans Index...")
        evans_results = calculate_evans_index(
            ventricle_segment['width'],
            skull_segment['width']
        )

        # æ•´åˆçµæœ
        results = {
            "dataset": dataset_name,
            "ventricle_segment": ventricle_segment,
            "skull_segment": skull_segment,
            "evans_analysis": evans_results,
            "files_used": {
                "original": paths["original"],
                "ventricles": ventricle_mask_path,
                "brain_mask": brain_mask_path,
                "ventricle_left": paths.get("ventricle_left", "N/A"),
                "ventricle_right": paths.get("ventricle_right", "N/A"),
                "needs_merge": paths["needs_merge"]
            }
        }

        # é¡¯ç¤ºçµæœ
        print(f"\nğŸ“Š {dataset_name} åˆ†æçµæœ:")
        print(f"   ğŸ§  Evans Index: {evans_results['evans_index']}")
        print(f"   ğŸ’¡ è‡¨åºŠæ„ç¾©: {evans_results['clinical_significance']}")
        print(f"   âš ï¸  è…¦æ°´è…«é¢¨éšª: {evans_results['hydrocephalus_risk']}")

        # é¡¯ç¤ºè­¦å‘Š
        if evans_results.get('warnings'):
            print(f"   âš ï¸  è­¦å‘Š: {', '.join(evans_results['warnings'])}")

        return results

    except Exception as e:
        print(f"âŒ åˆ†æ {dataset_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None

def batch_analyze_prelabeled_data(base_path: str, datasets: Optional[List[str]] = None) -> Dict:
    """
    æ‰¹æ¬¡åˆ†æå¤šå€‹æ¨™è¨˜è³‡æ–™é›†
    """
    if datasets is None:
        datasets = find_available_datasets(base_path)

    print(f"ğŸš€ é–‹å§‹æ‰¹æ¬¡åˆ†æ {len(datasets)} å€‹è³‡æ–™é›†...")

    results = {}
    successful = 0
    failed = 0

    for dataset in datasets:
        result = run_prelabeled_evans_analysis(base_path, dataset)
        if result:
            results[dataset] = result
            successful += 1
        else:
            failed += 1

    # ç¸½çµ
    print(f"\nğŸ“ˆ æ‰¹æ¬¡åˆ†æå®Œæˆ:")
    print(f"   âœ… æˆåŠŸ: {successful} å€‹")
    print(f"   âŒ å¤±æ•—: {failed} å€‹")

    # é¡¯ç¤ºçµ±è¨ˆæ‘˜è¦
    if results:
        evans_indices = [r["evans_analysis"]["evans_index"] for r in results.values()]
        avg_evans = np.mean(evans_indices)
        hydrocephalus_count = sum(1 for r in results.values()
                                if r["evans_analysis"]["hydrocephalus_risk"] == "é«˜")

        print(f"\nğŸ“Š çµ±è¨ˆæ‘˜è¦:")
        print(f"   å¹³å‡ Evans Index: {avg_evans:.4f}")
        print(f"   å¯èƒ½è…¦æ°´è…«æ¡ˆä¾‹: {hydrocephalus_count}/{len(results)} ({hydrocephalus_count/len(results)*100:.1f}%)")

    return results

def load_hydrocephalus_reference() -> List[str]:
    """
    è¼‰å…¥å·²çŸ¥æ°´è…¦ç—‡æ¡ˆä¾‹çš„åƒè€ƒæ¸…å–®
    """
    try:
        reference_file = "hydrocephalus_reference.json"
        if os.path.exists(reference_file):
            with open(reference_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get("hydrocephalus_cases", {}).get("cases", [])
        return []
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è¼‰å…¥æ°´è…¦ç—‡åƒè€ƒæ¸…å–®: {e}")
        return []

def validate_results_against_reference(results: Dict, known_hydrocephalus: List[str]) -> Dict:
    """
    é©—è­‰åˆ†æçµæœèˆ‡å·²çŸ¥è‡¨åºŠè¨ºæ–·çš„ä¸€è‡´æ€§
    """
    validation = {
        "total_analyzed": len(results),
        "known_hydrocephalus_count": len(known_hydrocephalus),
        "hydrocephalus_correctly_identified": 0,
        "normal_correctly_identified": 0,
        "false_negatives": [],  # æ‡‰è©²æ˜¯æ°´è…¦ç—‡ä½†è¢«åˆ¤ç‚ºæ­£å¸¸
        "false_positives": [],  # æ‡‰è©²æ˜¯æ­£å¸¸ä½†è¢«åˆ¤ç‚ºæ°´è…¦ç—‡
        "not_analyzed": [],
        "accuracy": 0.0
    }

    total_correct = 0

    # æª¢æŸ¥å·²çŸ¥æ°´è…¦ç—‡æ¡ˆä¾‹
    for case in known_hydrocephalus:
        if case in results:
            if results[case]["evans_analysis"]["hydrocephalus_risk"] == "é«˜":
                validation["hydrocephalus_correctly_identified"] += 1
                total_correct += 1
            else:
                validation["false_negatives"].append({
                    "case": case,
                    "evans_index": results[case]["evans_analysis"]["evans_index"]
                })
        else:
            validation["not_analyzed"].append(case)

    # æª¢æŸ¥æ‡‰è©²æ˜¯æ­£å¸¸çš„æ¡ˆä¾‹
    for case, result in results.items():
        if case not in known_hydrocephalus:  # æ‡‰è©²æ˜¯æ­£å¸¸æ¡ˆä¾‹
            if result["evans_analysis"]["hydrocephalus_risk"] == "ä½":
                validation["normal_correctly_identified"] += 1
                total_correct += 1
            else:
                validation["false_positives"].append({
                    "case": case,
                    "evans_index": result["evans_analysis"]["evans_index"]
                })

    # è¨ˆç®—æº–ç¢ºç‡
    if validation["total_analyzed"] > 0:
        validation["accuracy"] = total_correct / validation["total_analyzed"]

    return validation

def generate_markdown_report(results: Dict, output_file: str):
    """
    ç”¢ç”Ÿç°¡æ½”æ˜ç­çš„ Markdown å ±å‘Š
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# Evans Index åˆ†æå ±å‘Š\n\n")
        f.write(f"ğŸ“… åˆ†ææ™‚é–“: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        if not results:
            f.write("âŒ æ²’æœ‰æˆåŠŸåˆ†æçš„è³‡æ–™é›†\n")
            return

        # çµ±è¨ˆæ‘˜è¦
        evans_indices = [r["evans_analysis"]["evans_index"] for r in results.values()]
        avg_evans = sum(evans_indices) / len(evans_indices)
        hydrocephalus_count = sum(1 for r in results.values()
                                if r["evans_analysis"]["hydrocephalus_risk"] == "é«˜")

        f.write("## ğŸ“Š çµ±è¨ˆæ‘˜è¦\n\n")
        f.write(f"- **ç¸½å…±åˆ†ææ¡ˆä¾‹**: {len(results)} å€‹\n")
        f.write(f"- **å¹³å‡ Evans Index**: {avg_evans:.4f}\n")
        f.write(f"- **å¯èƒ½è…¦æ°´è…«æ¡ˆä¾‹**: {hydrocephalus_count}/{len(results)} ({hydrocephalus_count/len(results)*100:.1f}%)\n\n")

        # åˆ†é¡çµ±è¨ˆ
        normal_cases = [k for k, v in results.items() if v["evans_analysis"]["hydrocephalus_risk"] == "ä½"]
        risk_cases = [k for k, v in results.items() if v["evans_analysis"]["hydrocephalus_risk"] == "é«˜"]

        f.write("## ğŸŸ¢ æ­£å¸¸ç¯„åœæ¡ˆä¾‹\n\n")
        if normal_cases:
            f.write("| æ¡ˆä¾‹ | Evans Index | è…¦å®¤å¯¬åº¦ | é¡±éª¨å¯¬åº¦ |\n")
            f.write("|------|-------------|----------|----------|\n")
            for case in sorted(normal_cases):
                r = results[case]
                ei = r["evans_analysis"]["evans_index"]
                vw = r["evans_analysis"]["ventricle_width"]
                sw = r["evans_analysis"]["skull_width"]
                f.write(f"| {case} | {ei:.4f} | {vw} | {sw} |\n")
        else:
            f.write("æ²’æœ‰æ­£å¸¸ç¯„åœçš„æ¡ˆä¾‹\n")

        f.write("\n## ğŸ”´ é«˜é¢¨éšªæ¡ˆä¾‹\n\n")
        if risk_cases:
            f.write("| æ¡ˆä¾‹ | Evans Index | è…¦å®¤å¯¬åº¦ | é¡±éª¨å¯¬åº¦ | è‡¨åºŠæ„ç¾© |\n")
            f.write("|------|-------------|----------|----------|----------|\n")
            for case in sorted(risk_cases):
                r = results[case]
                ei = r["evans_analysis"]["evans_index"]
                vw = r["evans_analysis"]["ventricle_width"]
                sw = r["evans_analysis"]["skull_width"]
                cs = r["evans_analysis"]["clinical_significance"]
                f.write(f"| {case} | {ei:.4f} | {vw} | {sw} | {cs} |\n")
        else:
            f.write("æ²’æœ‰é«˜é¢¨éšªæ¡ˆä¾‹\n")


        # èªªæ˜
        f.write("## ğŸ“– èªªæ˜\n\n")
        f.write("- **Evans Index**: è…¦å®¤å¯¬åº¦èˆ‡é¡±éª¨å¯¬åº¦çš„æ¯”å€¼\n")
        f.write("- **æ­£å¸¸ç¯„åœ**: â‰¤ 0.3\n")
        f.write("- **è…¦æ°´è…«é¢¨éšª**: > 0.3 è¡¨ç¤ºå¯èƒ½æœ‰è…¦æ°´è…«\n")
        f.write("- **æ¸¬é‡æ–¹æ³•**: åœ¨ç›¸åŒ Z åˆ‡ç‰‡ä¸Šæ¸¬é‡è…¦å®¤å’Œé¡±éª¨çš„æœ€å¤§å¯¬åº¦\n\n")

if __name__ == "__main__":
    # è¨­å®šæ¨™è¨˜è³‡æ–™çš„è·¯å¾‘
    LABELED_DATA_PATH = "/Volumes/Kuroé†¬ã®1TSSD/æ¨™è¨˜å¥½çš„è³‡æ–™"

    if not os.path.exists(LABELED_DATA_PATH):
        print(f"âŒ æ‰¾ä¸åˆ°æ¨™è¨˜è³‡æ–™è·¯å¾‘: {LABELED_DATA_PATH}")
        exit(1)

    # æ‰¾å‡ºæ‰€æœ‰å¯ç”¨çš„è³‡æ–™é›†
    available_datasets = find_available_datasets(LABELED_DATA_PATH)
    print(f"ğŸ” ç™¼ç¾ {len(available_datasets)} å€‹è³‡æ–™é›†: {available_datasets}")

    # åŸ·è¡Œæ‰¹æ¬¡åˆ†æ
    batch_results = batch_analyze_prelabeled_data(LABELED_DATA_PATH)

    # è¼‰å…¥å·²çŸ¥æ°´è…¦ç—‡åƒè€ƒæ¸…å–®
    known_hydrocephalus = load_hydrocephalus_reference()
    if known_hydrocephalus:
        print(f"\nğŸ“‹ è¼‰å…¥å·²çŸ¥æ°´è…¦ç—‡æ¡ˆä¾‹: {len(known_hydrocephalus)} å€‹")

        # é©—è­‰çµæœ
        validation = validate_results_against_reference(batch_results, known_hydrocephalus)

        print(f"\nğŸ” é©—è­‰çµæœ:")
        print(f"   ç¸½é«”æº–ç¢ºç‡: {validation['accuracy']:.1%}")
        print(f"   æ°´è…¦ç—‡æ­£ç¢ºè­˜åˆ¥: {validation['hydrocephalus_correctly_identified']}/{validation['known_hydrocephalus_count']}")
        print(f"   æ­£å¸¸æ¡ˆä¾‹æ­£ç¢ºè­˜åˆ¥: {validation['normal_correctly_identified']}")

        if validation['false_negatives']:
            print(f"   æ¼å ±æ¡ˆä¾‹ (æ‡‰ç‚ºæ°´è…¦ç—‡ä½†åˆ¤ç‚ºæ­£å¸¸): {len(validation['false_negatives'])} å€‹")
            for fn in validation['false_negatives']:
                print(f"     - {fn['case']}: Evans Index = {fn['evans_index']}")

        if validation['false_positives']:
            print(f"   èª¤å ±æ¡ˆä¾‹ (æ‡‰ç‚ºæ­£å¸¸ä½†åˆ¤ç‚ºæ°´è…¦ç—‡): {len(validation['false_positives'])} å€‹")
            for fp in validation['false_positives']:
                print(f"     - {fp['case']}: Evans Index = {fp['evans_index']}")

        if validation['not_analyzed']:
            print(f"   æœªåˆ†ææ¡ˆä¾‹: {validation['not_analyzed']}")

    # å»ºç«‹çµæœè³‡æ–™å¤¾
    result_dir = "result"
    os.makedirs(result_dir, exist_ok=True)
    print(f"\nğŸ“ å»ºç«‹çµæœè³‡æ–™å¤¾: {result_dir}")

    # ä¿å­˜çµæœåˆ° JSON æª”æ¡ˆ
    output_file = os.path.join(result_dir, "prelabeled_evans_analysis_results.json")

    # å°‡é©—è­‰çµæœåŠ å…¥ JSON
    final_results = {
        "analysis_results": batch_results,
        "validation": validation if known_hydrocephalus else None,
        "known_hydrocephalus_cases": known_hydrocephalus
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ JSON çµæœå·²ä¿å­˜åˆ°: {output_file}")

    # ç”¢ç”Ÿ Markdown å ±å‘Š
    md_file = os.path.join(result_dir, "prelabeled_evans_analysis_report.md")
    generate_markdown_report(batch_results, md_file)
    print(f"ğŸ“„ Markdown å ±å‘Šå·²ä¿å­˜åˆ°: {md_file}")

    print(f"\nğŸ‰ æ‰€æœ‰çµæœæª”æ¡ˆå·²ä¿å­˜åœ¨ {result_dir}/ è³‡æ–™å¤¾ä¸­")
